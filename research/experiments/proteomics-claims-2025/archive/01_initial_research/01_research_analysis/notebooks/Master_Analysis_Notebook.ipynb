{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bioinformatics Finding Group Evaluation Framework\n",
    "\n",
    "## Comprehensive Analysis of Proteomic Data in Alzheimer's Disease Research\n",
    "\n",
    "This notebook provides a complete framework for evaluating biological statements using rigorous statistical methods on proteomic data from Alzheimer's disease cases.\n",
    "\n",
    "### Project Overview\n",
    "- **Dataset**: Mini-pools of 10 neurons from Alzheimer's disease cases\n",
    "- **Scope**: 5,853 proteins across neuronal samples\n",
    "- **Groups**: Two finding groups focused on mitochondrial dysregulation and proteostasis failure\n",
    "- **Methodology**: Statistical rigor following ISLP and Rosner's Biostatistics principles\n",
    "\n",
    "### Key Features\n",
    "- Enhanced documentation with analytical rationale\n",
    "- Comprehensive statistical testing with multiple correction methods\n",
    "- Advanced temporal analysis with sliding windows\n",
    "- AI automation capabilities for large-scale analysis\n",
    "- Publication-quality visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Setup and Dependencies\n",
    "\n",
    "### Required Libraries\n",
    "Before running this notebook, install the required dependencies:\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy scipy scanpy statsmodels scikit-learn matplotlib seaborn tqdm\n",
    "```\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, pearsonr, spearmanr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "sc.settings.verbosity = 3\n",
    "sc.settings.set_figure_params(dpi=100, facecolor='white')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"üìä Scanpy version: {sc.__version__}\")\n",
    "print(f\"üêº Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Data Loading and Exploration\n",
    "\n",
    "### Load the H5AD Dataset\n",
    "The dataset contains proteomic measurements from neuronal mini-pools with comprehensive metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate_data(file_path='data/pool_processed_v2.h5ad'):\n",
    "    \"\"\"\n",
    "    Load the H5AD file and perform comprehensive validation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    adata : AnnData\n",
    "        Loaded and validated dataset\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Loading proteomic dataset...\")\n",
    "    \n",
    "    # Load data with error handling\n",
    "    try:\n",
    "        adata = sc.read_h5ad(file_path)\n",
    "        print(f\"‚úÖ Successfully loaded dataset: {adata.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: Data file not found at {file_path}\")\n",
    "        print(\"üìÅ Please ensure the data file is in the correct location\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Dataset characteristics validation\n",
    "    print(\"\\nüìà Dataset Characteristics:\")\n",
    "    print(f\"   üì¶ Dimensions: {adata.n_obs} cells √ó {adata.n_vars} proteins\")\n",
    "    print(f\"   üíæ Memory usage: {adata.X.nbytes / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Metadata validation\n",
    "    print(\"\\nüóÇÔ∏è Metadata Columns:\")\n",
    "    for i, col in enumerate(adata.obs.columns):\n",
    "        print(f\"   {i+1:2d}. {col}\")\n",
    "    \n",
    "    # Required columns check\n",
    "    required_cols = ['tau_status', 'MC1', 'pseudotime']\n",
    "    missing = [col for col in required_cols if col not in adata.obs.columns]\n",
    "    if missing:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Missing required columns: {missing}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All required metadata columns present\")\n",
    "    \n",
    "    # Tau status distribution\n",
    "    if 'tau_status' in adata.obs.columns:\n",
    "        tau_counts = adata.obs['tau_status'].value_counts()\n",
    "        print(\"\\nüß† Tau Status Distribution:\")\n",
    "        for status, count in tau_counts.items():\n",
    "            percentage = 100 * count / len(adata.obs)\n",
    "            print(f\"   {status}: {count} cells ({percentage:.1f}%)\")\n",
    "    \n",
    "    # MC1 and pseudotime statistics\n",
    "    if 'MC1' in adata.obs.columns:\n",
    "        print(f\"\\nüî¨ MC1 Score Range: [{adata.obs['MC1'].min():.2f}, {adata.obs['MC1'].max():.2f}]\")\n",
    "        print(f\"   Mean ¬± SD: {adata.obs['MC1'].mean():.2f} ¬± {adata.obs['MC1'].std():.2f}\")\n",
    "    \n",
    "    if 'pseudotime' in adata.obs.columns:\n",
    "        print(f\"\\n‚è∞ Pseudotime Range: [{adata.obs['pseudotime'].min():.3f}, {adata.obs['pseudotime'].max():.3f}]\")\n",
    "        print(f\"   Mean ¬± SD: {adata.obs['pseudotime'].mean():.3f} ¬± {adata.obs['pseudotime'].std():.3f}\")\n",
    "    \n",
    "    # Sample protein names\n",
    "    print(f\"\\nüß¨ Sample Proteins: {', '.join(adata.var_names[:10].tolist())}...\")\n",
    "    \n",
    "    return adata\n",
    "\n",
    "# Load the dataset\n",
    "adata = load_and_validate_data()\n",
    "\n",
    "if adata is not None:\n",
    "    print(\"\\nüéâ Data loading completed successfully!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Failed to load data. Please check file path and dependencies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if adata is not None:\n",
    "    # Create comprehensive visualization of dataset characteristics\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Tau status distribution\n",
    "    if 'tau_status' in adata.obs.columns:\n",
    "        tau_counts = adata.obs['tau_status'].value_counts()\n",
    "        axes[0, 0].pie(tau_counts.values, labels=tau_counts.index, autopct='%1.1f%%', \n",
    "                      colors=['lightblue', 'lightcoral'])\n",
    "        axes[0, 0].set_title('Tau Status Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 2. MC1 score distribution\n",
    "    if 'MC1' in adata.obs.columns:\n",
    "        axes[0, 1].hist(adata.obs['MC1'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0, 1].axvline(adata.obs['MC1'].mean(), color='red', linestyle='--', \n",
    "                          label=f'Mean: {adata.obs[\"MC1\"].mean():.2f}')\n",
    "        axes[0, 1].set_xlabel('MC1 Score')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].set_title('MC1 Score Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Pseudotime distribution\n",
    "    if 'pseudotime' in adata.obs.columns:\n",
    "        axes[0, 2].hist(adata.obs['pseudotime'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        axes[0, 2].axvline(adata.obs['pseudotime'].mean(), color='red', linestyle='--',\n",
    "                          label=f'Mean: {adata.obs[\"pseudotime\"].mean():.3f}')\n",
    "        axes[0, 2].set_xlabel('Pseudotime')\n",
    "        axes[0, 2].set_ylabel('Frequency')\n",
    "        axes[0, 2].set_title('Pseudotime Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[0, 2].legend()\n",
    "    \n",
    "    # 4. MC1 vs Pseudotime relationship\n",
    "    if 'MC1' in adata.obs.columns and 'pseudotime' in adata.obs.columns:\n",
    "        scatter = axes[1, 0].scatter(adata.obs['pseudotime'], adata.obs['MC1'], \n",
    "                                   c=adata.obs['tau_status'].astype('category').cat.codes,\n",
    "                                   alpha=0.6, cmap='coolwarm')\n",
    "        axes[1, 0].set_xlabel('Pseudotime')\n",
    "        axes[1, 0].set_ylabel('MC1 Score')\n",
    "        axes[1, 0].set_title('MC1 vs Pseudotime (colored by tau status)', fontsize=14, fontweight='bold')\n",
    "        plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    \n",
    "    # 5. Expression level distribution (sample)\n",
    "    if adata.n_vars > 0:\n",
    "        sample_expr = adata[:, adata.var_names[0]].X.flatten()\n",
    "        axes[1, 1].hist(sample_expr, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "        axes[1, 1].set_xlabel('Log2 Expression')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].set_title(f'Expression Distribution\\n({adata.var_names[0]})', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 6. Dataset summary statistics\n",
    "    axes[1, 2].axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    üìä DATASET SUMMARY\n",
    "    \n",
    "    üì¶ Dimensions: {adata.n_obs} √ó {adata.n_vars}\n",
    "    \n",
    "    üß† Tau Status:\n",
    "    {adata.obs['tau_status'].value_counts().to_string() if 'tau_status' in adata.obs.columns else 'N/A'}\n",
    "    \n",
    "    üî¨ MC1 Score:\n",
    "    Range: [{adata.obs['MC1'].min():.2f}, {adata.obs['MC1'].max():.2f}]\n",
    "    Mean: {adata.obs['MC1'].mean():.2f}\n",
    "    \n",
    "    ‚è∞ Pseudotime:\n",
    "    Range: [{adata.obs['pseudotime'].min():.3f}, {adata.obs['pseudotime'].max():.3f}]\n",
    "    Mean: {adata.obs['pseudotime'].mean():.3f}\n",
    "    \n",
    "    ‚úÖ Ready for Analysis!\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 2].text(0.1, 0.5, summary_text, fontsize=11, verticalalignment='center',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot create visualizations - data not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß¨ Group 1: Late-Stage Mitochondrial Dysregulation\n",
    "\n",
    "This section analyzes mitochondrial dysfunction and autophagy failure in late-stage disease progression.\n",
    "\n",
    "### Key Statements:\n",
    "1. **UPS Protein Analysis**: No significant alterations in UPS proteins\n",
    "2. **SQSTM1 Upregulation**: Massive 10.7-fold increase in autophagy receptor\n",
    "3. **Sliding Window Analysis**: Dynamic correlation changes over disease progression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement 1: UPS Protein Analysis\n",
    "\n",
    "**Claim**: Targeted analyses show no significant UPS protein alterations across tau-positive versus tau-negative neurons.\n",
    "\n",
    "**Analytical Approach**: Conservative evaluation using dual testing (parametric/non-parametric) with FDR correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ups_proteins(adata):\n",
    "    \"\"\"\n",
    "    Comprehensive UPS protein analysis with enhanced documentation\n",
    "    \n",
    "    This analysis evaluates whether UPS proteins show significant alterations\n",
    "    between tau-positive and tau-negative neurons using rigorous statistical methods.\n",
    "    \"\"\"\n",
    "    if adata is None:\n",
    "        print(\"‚ùå No data available for analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üî¨ UPS Protein Analysis - Statement 1\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Step 1: Identify UPS proteins using multiple strategies\n",
    "    print(\"\\nüîç Step 1: UPS Protein Identification\")\n",
    "    \n",
    "    # Pattern-based identification\n",
    "    ups_patterns = ['UB', 'PSM', 'PROT', 'UBA', 'USP']\n",
    "    pattern_proteins = []\n",
    "    for pattern in ups_patterns:\n",
    "        matches = [p for p in adata.var_names if pattern in p.upper()]\n",
    "        pattern_proteins.extend(matches)\n",
    "        print(f\"   {pattern}: {len(matches)} proteins found\")\n",
    "    \n",
    "    # Curated UPS protein list (from literature)\n",
    "    curated_ups = ['PSMA1', 'PSMA2', 'PSMA3', 'PSMA4', 'PSMA5', 'PSMA6', 'PSMA7',\n",
    "                   'PSMB1', 'PSMB2', 'PSMB3', 'PSMB4', 'PSMB5', 'PSMB6', 'PSMB7',\n",
    "                   'UBA1', 'UBA2', 'UBA3', 'UBE2A', 'UBE2B', 'UBE2C']\n",
    "    \n",
    "    available_curated = [p for p in curated_ups if p in adata.var_names]\n",
    "    print(f\"   Curated list: {len(available_curated)}/{len(curated_ups)} available\")\n",
    "    \n",
    "    # Combine and deduplicate\n",
    "    all_ups = list(set(pattern_proteins + available_curated))\n",
    "    print(f\"   üìä Total UPS proteins identified: {len(all_ups)}\")\n",
    "    \n",
    "    if len(all_ups) < 5:\n",
    "        print(\"‚ö†Ô∏è Warning: Few UPS proteins found. Analysis may be underpowered.\")\n",
    "    \n",
    "    # Step 2: Differential expression analysis\n",
    "    print(\"\\nüìà Step 2: Differential Expression Analysis\")\n",
    "    \n",
    "    if 'tau_status' not in adata.obs.columns:\n",
    "        print(\"‚ùå Error: tau_status column not found\")\n",
    "        return None\n",
    "    \n",
    "    # Split by tau status\n",
    "    tau_pos = adata[adata.obs['tau_status'] == 'positive']\n",
    "    tau_neg = adata[adata.obs['tau_status'] == 'negative']\n",
    "    \n",
    "    print(f\"   Tau-positive: {len(tau_pos)} cells\")\n",
    "    print(f\"   Tau-negative: {len(tau_neg)} cells\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"\\nüßÆ Calculating statistics for each UPS protein...\")\n",
    "    for protein in tqdm(all_ups, desc=\"Processing UPS proteins\"):\n",
    "        if protein not in adata.var_names:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Extract expression values\n",
    "            pos_expr = tau_pos[:, protein].X.flatten()\n",
    "            neg_expr = tau_neg[:, protein].X.flatten()\n",
    "            \n",
    "            # Calculate basic statistics\n",
    "            mean_pos = np.mean(pos_expr)\n",
    "            mean_neg = np.mean(neg_expr)\n",
    "            log2fc = mean_pos - mean_neg  # Already in log2 scale\n",
    "            \n",
    "            # Parametric test (t-test)\n",
    "            t_stat, p_ttest = ttest_ind(pos_expr, neg_expr)\n",
    "            \n",
    "            # Non-parametric test (Mann-Whitney U)\n",
    "            u_stat, p_mannwhitney = mannwhitneyu(pos_expr, neg_expr, alternative='two-sided')\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt(((len(pos_expr)-1)*np.var(pos_expr, ddof=1) + \n",
    "                                 (len(neg_expr)-1)*np.var(neg_expr, ddof=1)) / \n",
    "                                (len(pos_expr) + len(neg_expr) - 2))\n",
    "            cohens_d = (mean_pos - mean_neg) / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'protein': protein,\n",
    "                'mean_tau_pos': mean_pos,\n",
    "                'mean_tau_neg': mean_neg,\n",
    "                'log2fc': log2fc,\n",
    "                'p_ttest': p_ttest,\n",
    "                'p_mannwhitney': p_mannwhitney,\n",
    "                'cohens_d': cohens_d,\n",
    "                't_statistic': t_stat\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing {protein}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ùå No results generated. Check protein names and data.\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Step 3: Multiple testing correction\n",
    "    print(\"\\nüîß Step 3: Multiple Testing Correction\")\n",
    "    \n",
    "    # FDR correction for both test types\n",
    "    rejected_ttest, fdr_ttest, _, _ = multipletests(results_df['p_ttest'], method='fdr_bh')\n",
    "    rejected_mw, fdr_mw, _, _ = multipletests(results_df['p_mannwhitney'], method='fdr_bh')\n",
    "    \n",
    "    results_df['fdr_ttest'] = fdr_ttest\n",
    "    results_df['fdr_mannwhitney'] = fdr_mw\n",
    "    results_df['significant_ttest'] = rejected_ttest\n",
    "    results_df['significant_mannwhitney'] = rejected_mw\n",
    "    \n",
    "    # Combined significance (both tests significant)\n",
    "    results_df['significant_both'] = results_df['significant_ttest'] & results_df['significant_mannwhitney']\n",
    "    \n",
    "    # Step 4: Results summary\n",
    "    print(\"\\nüìä Step 4: Results Summary\")\n",
    "    \n",
    "    n_total = len(results_df)\n",
    "    n_sig_ttest = sum(results_df['significant_ttest'])\n",
    "    n_sig_mw = sum(results_df['significant_mannwhitney'])\n",
    "    n_sig_both = sum(results_df['significant_both'])\n",
    "    \n",
    "    pct_sig_ttest = 100 * n_sig_ttest / n_total\n",
    "    pct_sig_mw = 100 * n_sig_mw / n_total\n",
    "    pct_sig_both = 100 * n_sig_both / n_total\n",
    "    \n",
    "    print(f\"   üìà Total UPS proteins analyzed: {n_total}\")\n",
    "    print(f\"   üî¨ Significant (t-test, FDR<0.05): {n_sig_ttest}/{n_total} ({pct_sig_ttest:.1f}%)\")\n",
    "    print(f\"   üî¨ Significant (Mann-Whitney, FDR<0.05): {n_sig_mw}/{n_total} ({pct_sig_mw:.1f}%)\")\n",
    "    print(f\"   üî¨ Significant (both tests): {n_sig_both}/{n_total} ({pct_sig_both:.1f}%)\")\n",
    "    \n",
    "    # Effect size summary\n",
    "    mean_abs_d = np.mean(np.abs(results_df['cohens_d']))\n",
    "    print(f\"   üìè Mean absolute Cohen's d: {mean_abs_d:.3f}\")\n",
    "    \n",
    "    # Statement evaluation\n",
    "    print(\"\\nüéØ Statement Evaluation\")\n",
    "    \n",
    "    # Criteria for \"no significant alterations\"\n",
    "    criteria_met = pct_sig_both < 5 and mean_abs_d < 0.2\n",
    "    \n",
    "    if criteria_met:\n",
    "        evaluation = \"SUPPORTED\"\n",
    "        explanation = f\"Only {pct_sig_both:.1f}% of UPS proteins show significant alterations (both tests), with small effect sizes (mean |d|={mean_abs_d:.3f})\"\n",
    "    else:\n",
    "        evaluation = \"REFUTED\"\n",
    "        explanation = f\"{pct_sig_both:.1f}% of UPS proteins show significant alterations, or effect sizes are large (mean |d|={mean_abs_d:.3f})\"\n",
    "    \n",
    "    print(f\"   üìã Evaluation: {evaluation}\")\n",
    "    print(f\"   üìù Explanation: {explanation}\")\n",
    "    \n",
    "    return {\n",
    "        'results_df': results_df,\n",
    "        'evaluation': evaluation,\n",
    "        'explanation': explanation,\n",
    "        'n_total': n_total,\n",
    "        'pct_significant': pct_sig_both,\n",
    "        'mean_effect_size': mean_abs_d\n",
    "    }\n",
    "\n",
    "# Run UPS protein analysis\n",
    "if adata is not None:\n",
    "    ups_results = analyze_ups_proteins(adata)\n",
    "    \n",
    "    if ups_results:\n",
    "        print(f\"\\n‚úÖ UPS Analysis Complete: {ups_results['evaluation']}\")\n",
    "    else:\n",
    "        print(\"‚ùå UPS Analysis Failed\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping UPS analysis - no data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement 2: SQSTM1 Upregulation Analysis\n",
    "\n",
    "**Claim**: SQSTM1 shows 10.7-fold increase (2^3.413) between tau states, representing massive autophagy dysfunction.\n",
    "\n",
    "**Analytical Approach**: Bootstrap validation with multiple statistical measures for extreme fold change validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sqstm1_upregulation(adata):\n",
    "    \"\"\"\n",
    "    Comprehensive SQSTM1 upregulation analysis with bootstrap validation\n",
    "    \n",
    "    SQSTM1/p62 is the primary autophagy receptor protein that accumulates\n",
    "    when autophagy flux is impaired, making it a critical biomarker.\n",
    "    \"\"\"\n",
    "    if adata is None:\n",
    "        print(\"‚ùå No data available for analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üß¨ SQSTM1 Upregulation Analysis - Statement 2\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check if SQSTM1 exists\n",
    "    if 'SQSTM1' not in adata.var_names:\n",
    "        print(\"‚ùå SQSTM1 not found in dataset\")\n",
    "        # Look for alternatives\n",
    "        alternatives = [p for p in adata.var_names if 'SQSTM' in p.upper()]\n",
    "        if alternatives:\n",
    "            print(f\"üîç Potential alternatives found: {alternatives}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"‚úÖ SQSTM1 found in dataset\")\n",
    "    \n",
    "    # Step 1: Basic differential expression\n",
    "    print(\"\\nüìä Step 1: Differential Expression Analysis\")\n",
    "    \n",
    "    tau_pos = adata[adata.obs['tau_status'] == 'positive']\n",
    "    tau_neg = adata[adata.obs['tau_status'] == 'negative']\n",
    "    \n",
    "    pos_expr = tau_pos[:, 'SQSTM1'].X.flatten()\n",
    "    neg_expr = tau_neg[:, 'SQSTM1'].X.flatten()\n",
    "    \n",
    "    mean_pos = np.mean(pos_expr)\n",
    "    mean_neg = np.mean(neg_expr)\n",
    "    log2fc = mean_pos - mean_neg  # Data already in log2 scale\n",
    "    fold_change = 2**log2fc\n",
    "    \n",
    "    print(f\"   üìà Tau-positive mean: {mean_pos:.3f}\")\n",
    "    print(f\"   üìâ Tau-negative mean: {mean_neg:.3f}\")\n",
    "    print(f\"   üî¢ Log2 fold change: {log2fc:.3f}\")\n",
    "    print(f\"   üìä Fold change: {fold_change:.1f}x\")\n",
    "    print(f\"   üéØ Expected: 3.413 log2FC (10.7x)\")\n",
    "    \n",
    "    # Statistical test\n",
    "    t_stat, p_val = ttest_ind(pos_expr, neg_expr)\n",
    "    print(f\"   üìê T-statistic: {t_stat:.3f}\")\n",
    "    print(f\"   üé≤ P-value: {p_val:.2e}\")\n",
    "    \n",
    "    # Step 2: Bootstrap confidence interval\n",
    "    print(\"\\nüîÑ Step 2: Bootstrap Confidence Interval\")\n",
    "    \n",
    "    n_bootstrap = 1000\n",
    "    bootstrap_fc = []\n",
    "    \n",
    "    print(f\"   Running {n_bootstrap} bootstrap iterations...\")\n",
    "    for i in range(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        pos_sample = np.random.choice(pos_expr, len(pos_expr), replace=True)\n",
    "        neg_sample = np.random.choice(neg_expr, len(neg_expr), replace=True)\n",
    "        \n",
    "        # Calculate fold change\n",
    "        boot_fc = np.mean(pos_sample) - np.mean(neg_sample)\n",
    "        bootstrap_fc.append(boot_fc)\n",
    "    \n",
    "    # Confidence intervals\n",
    "    ci_lower = np.percentile(bootstrap_fc, 2.5)\n",
    "    ci_upper = np.percentile(bootstrap_fc, 97.5)\n",
    "    \n",
    "    print(f\"   üìä Bootstrap 95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
    "    print(f\"   üéØ Expected 3.413 in CI: {'Yes' if ci_lower <= 3.413 <= ci_upper else 'No'}\")\n",
    "    \n",
    "    # Step 3: Pseudotime correlation\n",
    "    print(\"\\n‚è∞ Step 3: Pseudotime Correlation Analysis\")\n",
    "    \n",
    "    if 'pseudotime' in adata.obs.columns:\n",
    "        sqstm1_all = adata[:, 'SQSTM1'].X.flatten()\n",
    "        pseudotime = adata.obs['pseudotime'].values\n",
    "        \n",
    "        # Correlation\n",
    "        corr, corr_p = pearsonr(sqstm1_all, pseudotime)\n",
    "        print(f\"   üìà Correlation with pseudotime: r={corr:.3f}, p={corr_p:.2e}\")\n",
    "        \n",
    "        # Linear regression for Œ≤ coefficient\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(pseudotime.reshape(-1, 1), sqstm1_all)\n",
    "        beta = lr.coef_[0]\n",
    "        r_squared = lr.score(pseudotime.reshape(-1, 1), sqstm1_all)\n",
    "        \n",
    "        print(f\"   üìê Linear regression Œ≤: {beta:.3f}\")\n",
    "        print(f\"   üìä R-squared: {r_squared:.3f}\")\n",
    "        print(f\"   üéØ Expected Œ≤ ‚âà 4.951\")\n",
    "    \n",
    "    # Step 4: Visualization\n",
    "    print(\"\\nüìà Step 4: Creating Visualizations\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Expression comparison\n",
    "    axes[0, 0].boxplot([neg_expr, pos_expr], labels=['Tau-negative', 'Tau-positive'])\n",
    "    axes[0, 0].set_ylabel('SQSTM1 Expression (log2)')\n",
    "    axes[0, 0].set_title('SQSTM1 Expression by Tau Status')\n",
    "    \n",
    "    # 2. Bootstrap distribution\n",
    "    axes[0, 1].hist(bootstrap_fc, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 1].axvline(log2fc, color='red', linestyle='-', linewidth=2, label=f'Observed: {log2fc:.3f}')\n",
    "    axes[0, 1].axvline(3.413, color='orange', linestyle='--', linewidth=2, label='Expected: 3.413')\n",
    "    axes[0, 1].axvline(ci_lower, color='gray', linestyle=':', label=f'95% CI')\n",
    "    axes[0, 1].axvline(ci_upper, color='gray', linestyle=':')\n",
    "    axes[0, 1].set_xlabel('Log2 Fold Change')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Bootstrap Distribution of Fold Changes')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Pseudotime correlation\n",
    "    if 'pseudotime' in adata.obs.columns:\n",
    "        scatter = axes[1, 0].scatter(pseudotime, sqstm1_all, \n",
    "                                   c=adata.obs['tau_status'].astype('category').cat.codes,\n",
    "                                   alpha=0.6, cmap='coolwarm')\n",
    "        \n",
    "        # Add regression line\n",
    "        x_line = np.linspace(pseudotime.min(), pseudotime.max(), 100)\n",
    "        y_line = lr.predict(x_line.reshape(-1, 1))\n",
    "        axes[1, 0].plot(x_line, y_line, 'black', linewidth=2, \n",
    "                       label=f'Œ≤={beta:.3f}, R¬≤={r_squared:.3f}')\n",
    "        \n",
    "        axes[1, 0].set_xlabel('Pseudotime')\n",
    "        axes[1, 0].set_ylabel('SQSTM1 Expression')\n",
    "        axes[1, 0].set_title('SQSTM1 vs Pseudotime')\n",
    "        axes[1, 0].legend()\n",
    "        plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    \n",
    "    # 4. Summary statistics\n",
    "    axes[1, 1].axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    SQSTM1 ANALYSIS SUMMARY\n",
    "    \n",
    "    üìä Fold Change Analysis:\n",
    "    Observed: {log2fc:.3f} log2FC ({fold_change:.1f}x)\n",
    "    Expected: 3.413 log2FC (10.7x)\n",
    "    \n",
    "    üî¨ Statistical Test:\n",
    "    T-statistic: {t_stat:.3f}\n",
    "    P-value: {p_val:.2e}\n",
    "    \n",
    "    üîÑ Bootstrap 95% CI:\n",
    "    [{ci_lower:.3f}, {ci_upper:.3f}]\n",
    "    \n",
    "    ‚è∞ Pseudotime Correlation:\n",
    "    r = {corr:.3f} (p = {corr_p:.2e})\n",
    "    Œ≤ = {beta:.3f} (expected ‚âà 4.951)\n",
    "    \n",
    "    üéØ EVALUATION:\n",
    "    {'SUPPORTED' if abs(log2fc - 3.413) < 0.5 else 'NEEDS REVIEW'}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.5, summary_text, fontsize=10, verticalalignment='center',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Step 5: Evaluation\n",
    "    print(\"\\nüéØ Step 5: Statement Evaluation\")\n",
    "    \n",
    "    # Criteria for supporting the claim\n",
    "    fc_match = abs(log2fc - 3.413) < 0.5  # Within 0.5 log2 units\n",
    "    significant = p_val < 0.05\n",
    "    ci_contains_expected = ci_lower <= 3.413 <= ci_upper\n",
    "    \n",
    "    if fc_match and significant:\n",
    "        evaluation = \"SUPPORTED\"\n",
    "        explanation = f\"SQSTM1 shows {log2fc:.3f} log2FC (observed) vs 3.413 (expected), statistically significant (p={p_val:.2e})\"\n",
    "    elif significant:\n",
    "        evaluation = \"PARTIALLY SUPPORTED\"\n",
    "        explanation = f\"SQSTM1 significantly upregulated ({log2fc:.3f} log2FC) but differs from expected 3.413\"\n",
    "    else:\n",
    "        evaluation = \"REFUTED\"\n",
    "        explanation = f\"SQSTM1 fold change not statistically significant (p={p_val:.3f})\"\n",
    "    \n",
    "    print(f\"   üìã Evaluation: {evaluation}\")\n",
    "    print(f\"   üìù Explanation: {explanation}\")\n",
    "    \n",
    "    return {\n",
    "        'log2fc_observed': log2fc,\n",
    "        'log2fc_expected': 3.413,\n",
    "        'fold_change': fold_change,\n",
    "        'p_value': p_val,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'pseudotime_correlation': corr if 'pseudotime' in adata.obs.columns else None,\n",
    "        'beta_coefficient': beta if 'pseudotime' in adata.obs.columns else None,\n",
    "        'evaluation': evaluation,\n",
    "        'explanation': explanation\n",
    "    }\n",
    "\n",
    "# Run SQSTM1 analysis\n",
    "if adata is not None:\n",
    "    sqstm1_results = analyze_sqstm1_upregulation(adata)\n",
    "    \n",
    "    if sqstm1_results:\n",
    "        print(f\"\\n‚úÖ SQSTM1 Analysis Complete: {sqstm1_results['evaluation']}\")\n",
    "    else:\n",
    "        print(\"‚ùå SQSTM1 Analysis Failed\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping SQSTM1 analysis - no data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement 6: Sliding Window Correlation Analysis\n",
    "\n",
    "**Claim**: Running correlation between SQSTM1 and VDAC1 shifts from negative early (r=-0.417) to positive late (r=0.478) with strong trend (r=0.851).\n",
    "\n",
    "**Analytical Approach**: Advanced temporal analysis revealing dynamic relationship changes during disease progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_correlation_analysis(adata, protein1='SQSTM1', protein2='VDAC1', window_size=20):\n",
    "    \"\"\"\n",
    "    Advanced sliding window correlation analysis\n",
    "    \n",
    "    This analysis reveals temporal dynamics that static correlations miss,\n",
    "    showing how protein relationships change during disease progression.\n",
    "    \"\"\"\n",
    "    if adata is None:\n",
    "        print(\"‚ùå No data available for analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üìä Sliding Window Correlation Analysis - Statement 6\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check proteins exist\n",
    "    missing_proteins = [p for p in [protein1, protein2] if p not in adata.var_names]\n",
    "    if missing_proteins:\n",
    "        print(f\"‚ùå Missing proteins: {missing_proteins}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ Analyzing {protein1} vs {protein2} correlation\")\n",
    "    print(f\"üìè Window size: {window_size} cells\")\n",
    "    \n",
    "    # Check pseudotime\n",
    "    if 'pseudotime' not in adata.obs.columns:\n",
    "        print(\"‚ùå Pseudotime column not found\")\n",
    "        return None\n",
    "    \n",
    "    # Step 1: Sort by pseudotime and prepare data\n",
    "    print(\"\\nüîÑ Step 1: Data Preparation\")\n",
    "    \n",
    "    sorted_idx = np.argsort(adata.obs['pseudotime'])\n",
    "    adata_sorted = adata[sorted_idx]\n",
    "    \n",
    "    expr1 = adata_sorted[:, protein1].X.flatten()\n",
    "    expr2 = adata_sorted[:, protein2].X.flatten()\n",
    "    pseudotime_sorted = adata_sorted.obs['pseudotime'].values\n",
    "    \n",
    "    print(f\"   üìä Total cells: {len(adata_sorted)}\")\n",
    "    print(f\"   ‚è∞ Pseudotime range: [{pseudotime_sorted.min():.3f}, {pseudotime_sorted.max():.3f}]\")\n",
    "    \n",
    "    # Step 2: Sliding window calculation\n",
    "    print(\"\\nüìà Step 2: Sliding Window Calculation\")\n",
    "    \n",
    "    correlations = []\n",
    "    positions = []\n",
    "    p_values = []\n",
    "    window_info = []\n",
    "    \n",
    "    n_windows = len(adata_sorted) - window_size + 1\n",
    "    print(f\"   üî¢ Number of windows: {n_windows}\")\n",
    "    \n",
    "    for i in tqdm(range(n_windows), desc=\"Processing windows\"):\n",
    "        # Extract window data\n",
    "        window_expr1 = expr1[i:i+window_size]\n",
    "        window_expr2 = expr2[i:i+window_size]\n",
    "        window_pseudotime = pseudotime_sorted[i:i+window_size]\n",
    "        \n",
    "        # Calculate correlation\n",
    "        if np.std(window_expr1) > 0 and np.std(window_expr2) > 0:\n",
    "            corr, p_val = pearsonr(window_expr1, window_expr2)\n",
    "            \n",
    "            correlations.append(corr)\n",
    "            positions.append(np.mean(window_pseudotime))\n",
    "            p_values.append(p_val)\n",
    "            \n",
    "            window_info.append({\n",
    "                'start_idx': i,\n",
    "                'end_idx': i + window_size - 1,\n",
    "                'pseudotime_min': np.min(window_pseudotime),\n",
    "                'pseudotime_max': np.max(window_pseudotime),\n",
    "                'pseudotime_mean': np.mean(window_pseudotime),\n",
    "                'correlation': corr,\n",
    "                'p_value': p_val\n",
    "            })\n",
    "    \n",
    "    correlations = np.array(correlations)\n",
    "    positions = np.array(positions)\n",
    "    p_values = np.array(p_values)\n",
    "    \n",
    "    print(f\"   ‚úÖ Computed {len(correlations)} window correlations\")\n",
    "    \n",
    "    # Step 3: Phase analysis\n",
    "    print(\"\\nüîç Step 3: Phase Analysis\")\n",
    "    \n",
    "    # Define phases based on pseudotime\n",
    "    early_mask = positions < 0.33\n",
    "    late_mask = positions > 0.67\n",
    "    middle_mask = (positions >= 0.33) & (positions <= 0.67)\n",
    "    \n",
    "    early_corrs = correlations[early_mask]\n",
    "    late_corrs = correlations[late_mask]\n",
    "    middle_corrs = correlations[middle_mask]\n",
    "    \n",
    "    print(f\"   üåÖ Early phase (pseudotime < 0.33): {len(early_corrs)} windows\")\n",
    "    print(f\"   üåá Late phase (pseudotime > 0.67): {len(late_corrs)} windows\")\n",
    "    print(f\"   üå§Ô∏è Middle phase: {len(middle_corrs)} windows\")\n",
    "    \n",
    "    if len(early_corrs) > 0:\n",
    "        early_mean = np.mean(early_corrs)\n",
    "        early_std = np.std(early_corrs)\n",
    "        print(f\"   üìä Early mean correlation: {early_mean:.3f} ¬± {early_std:.3f}\")\n",
    "        print(f\"   üéØ Expected early: -0.417\")\n",
    "    \n",
    "    if len(late_corrs) > 0:\n",
    "        late_mean = np.mean(late_corrs)\n",
    "        late_std = np.std(late_corrs)\n",
    "        print(f\"   üìä Late mean correlation: {late_mean:.3f} ¬± {late_std:.3f}\")\n",
    "        print(f\"   üéØ Expected late: 0.478\")\n",
    "    \n",
    "    # Step 4: Trend analysis\n",
    "    print(\"\\nüìà Step 4: Trend Analysis\")\n",
    "    \n",
    "    if len(correlations) > 2:\n",
    "        trend_corr, trend_p = pearsonr(positions, correlations)\n",
    "        \n",
    "        # Linear regression for trend\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(positions.reshape(-1, 1), correlations)\n",
    "        trend_slope = lr.coef_[0]\n",
    "        trend_r2 = lr.score(positions.reshape(-1, 1), correlations)\n",
    "        \n",
    "        print(f\"   üìà Trend correlation: r={trend_corr:.3f}, p={trend_p:.2e}\")\n",
    "        print(f\"   üìê Trend slope: {trend_slope:.3f}\")\n",
    "        print(f\"   üìä Trend R¬≤: {trend_r2:.3f}\")\n",
    "        print(f\"   üéØ Expected trend: r=0.851, p=6.98e-08\")\n",
    "    \n",
    "    # Step 5: Comprehensive visualization\n",
    "    print(\"\\nüìä Step 5: Creating Comprehensive Visualizations\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # 1. Main sliding window plot\n",
    "    colors = plt.cm.coolwarm((correlations + 1) / 2)  # Normalize to [0, 1]\n",
    "    scatter = axes[0, 0].scatter(positions, correlations, c=correlations,\n",
    "                               cmap='coolwarm', vmin=-1, vmax=1, s=30, alpha=0.7)\n",
    "    \n",
    "    # Phase boundaries\n",
    "    axes[0, 0].axvline(x=0.33, color='gray', linestyle='--', alpha=0.5, label='Phase boundaries')\n",
    "    axes[0, 0].axvline(x=0.67, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Expected values\n",
    "    axes[0, 0].axhline(y=-0.417, color='blue', linestyle=':', alpha=0.7, label='Expected early')\n",
    "    axes[0, 0].axhline(y=0.478, color='red', linestyle=':', alpha=0.7, label='Expected late')\n",
    "    \n",
    "    # Trend line\n",
    "    if len(correlations) > 2:\n",
    "        x_trend = np.linspace(positions.min(), positions.max(), 100)\n",
    "        y_trend = lr.predict(x_trend.reshape(-1, 1))\n",
    "        axes[0, 0].plot(x_trend, y_trend, 'black', linewidth=2, \n",
    "                       label=f'Trend (r={trend_corr:.3f})')\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Pseudotime')\n",
    "    axes[0, 0].set_ylabel('Correlation')\n",
    "    axes[0, 0].set_title(f'{protein1}-{protein2} Sliding Window Correlation')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[0, 0], label='Correlation')\n",
    "    \n",
    "    # 2. Phase comparison boxplot\n",
    "    phase_data = []\n",
    "    phase_labels = []\n",
    "    \n",
    "    if len(early_corrs) > 0:\n",
    "        phase_data.append(early_corrs)\n",
    "        phase_labels.append(f'Early\\n(n={len(early_corrs)})')\n",
    "    if len(middle_corrs) > 0:\n",
    "        phase_data.append(middle_corrs)\n",
    "        phase_labels.append(f'Middle\\n(n={len(middle_corrs)})')\n",
    "    if len(late_corrs) > 0:\n",
    "        phase_data.append(late_corrs)\n",
    "        phase_labels.append(f'Late\\n(n={len(late_corrs)})')\n",
    "    \n",
    "    if phase_data:\n",
    "        bp = axes[0, 1].boxplot(phase_data, labels=phase_labels, patch_artist=True)\n",
    "        colors = ['lightblue', 'lightgray', 'lightcoral']\n",
    "        for patch, color in zip(bp['boxes'], colors[:len(bp['boxes'])]):\n",
    "            patch.set_facecolor(color)\n",
    "    \n",
    "    axes[0, 1].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    axes[0, 1].set_ylabel('Correlation')\n",
    "    axes[0, 1].set_title('Correlation by Phase')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Correlation distribution\n",
    "    axes[0, 2].hist(correlations, bins=25, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 2].axvline(x=np.mean(correlations), color='red', linestyle='--',\n",
    "                      label=f'Mean: {np.mean(correlations):.3f}')\n",
    "    axes[0, 2].axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "    axes[0, 2].set_xlabel('Correlation')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].set_title('Distribution of Window Correlations')\n",
    "    axes[0, 2].legend()\n",
    "    \n",
    "    # 4. P-value distribution\n",
    "    axes[1, 0].hist(p_values, bins=25, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1, 0].axvline(x=0.05, color='red', linestyle='--', label='p = 0.05')\n",
    "    sig_pct = 100 * sum(np.array(p_values) < 0.05) / len(p_values)\n",
    "    axes[1, 0].set_xlabel('P-value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title(f'P-value Distribution\\n({sig_pct:.1f}% significant)')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 5. Smoothed trajectory\n",
    "    if len(correlations) > 10:\n",
    "        # Moving average smooth\n",
    "        window_smooth = min(10, len(correlations)//5)\n",
    "        smoothed = pd.Series(correlations).rolling(window=window_smooth, center=True).mean()\n",
    "        axes[1, 1].plot(positions, correlations, 'lightgray', alpha=0.5, label='Raw')\n",
    "        axes[1, 1].plot(positions, smoothed, 'blue', linewidth=2, \n",
    "                       label=f'Smoothed (window={window_smooth})')\n",
    "    else:\n",
    "        axes[1, 1].plot(positions, correlations, 'blue', linewidth=2, label='Correlations')\n",
    "    \n",
    "    axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[1, 1].set_xlabel('Pseudotime')\n",
    "    axes[1, 1].set_ylabel('Correlation')\n",
    "    axes[1, 1].set_title('Smoothed Correlation Trajectory')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Summary statistics\n",
    "    axes[1, 2].axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    SLIDING WINDOW SUMMARY\n",
    "    \n",
    "    üìä Analysis Parameters:\n",
    "    Window size: {window_size}\n",
    "    Total windows: {len(correlations)}\n",
    "    \n",
    "    üåÖ Early Phase (< 0.33):\n",
    "    Mean r: {early_mean:.3f} (expected: -0.417)\n",
    "    \n",
    "    üåá Late Phase (> 0.67):\n",
    "    Mean r: {late_mean:.3f} (expected: 0.478)\n",
    "    \n",
    "    üìà Overall Trend:\n",
    "    r = {trend_corr:.3f} (expected: 0.851)\n",
    "    p = {trend_p:.2e} (expected: 6.98e-08)\n",
    "    \n",
    "    üéØ EVALUATION:\n",
    "    {'SUPPORTED' if (len(early_corrs) > 0 and len(late_corrs) > 0 and \n",
    "                    abs(early_mean - (-0.417)) < 0.15 and \n",
    "                    abs(late_mean - 0.478) < 0.15 and \n",
    "                    abs(trend_corr - 0.851) < 0.15) else 'NEEDS REVIEW'}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 2].text(0.05, 0.5, summary_text, fontsize=10, verticalalignment='center',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Step 6: Evaluation\n",
    "    print(\"\\nüéØ Step 6: Statement Evaluation\")\n",
    "    \n",
    "    # Evaluation criteria\n",
    "    criteria_met = []\n",
    "    criteria_failed = []\n",
    "    \n",
    "    if len(early_corrs) > 0:\n",
    "        early_match = abs(early_mean - (-0.417)) < 0.15\n",
    "        if early_match:\n",
    "            criteria_met.append(f\"Early phase correlation matches (-0.417 vs {early_mean:.3f})\")\n",
    "        else:\n",
    "            criteria_failed.append(f\"Early phase mismatch: {early_mean:.3f} vs -0.417\")\n",
    "    \n",
    "    if len(late_corrs) > 0:\n",
    "        late_match = abs(late_mean - 0.478) < 0.15\n",
    "        if late_match:\n",
    "            criteria_met.append(f\"Late phase correlation matches (0.478 vs {late_mean:.3f})\")\n",
    "        else:\n",
    "            criteria_failed.append(f\"Late phase mismatch: {late_mean:.3f} vs 0.478\")\n",
    "    \n",
    "    if len(correlations) > 2:\n",
    "        trend_match = abs(trend_corr - 0.851) < 0.15 and trend_p < 1e-6\n",
    "        if trend_match:\n",
    "            criteria_met.append(f\"Trend correlation matches (0.851 vs {trend_corr:.3f})\")\n",
    "        else:\n",
    "            criteria_failed.append(f\"Trend mismatch: {trend_corr:.3f} vs 0.851\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    if len(criteria_met) >= 2:\n",
    "        evaluation = \"SUPPORTED\"\n",
    "        explanation = f\"Key criteria met: {'; '.join(criteria_met)}\"\n",
    "    elif len(criteria_met) >= 1:\n",
    "        evaluation = \"PARTIALLY SUPPORTED\"\n",
    "        explanation = f\"Some criteria met: {'; '.join(criteria_met)}. Issues: {'; '.join(criteria_failed)}\"\n",
    "    else:\n",
    "        evaluation = \"REFUTED\"\n",
    "        explanation = f\"Criteria not met: {'; '.join(criteria_failed)}\"\n",
    "    \n",
    "    print(f\"   üìã Evaluation: {evaluation}\")\n",
    "    print(f\"   üìù Explanation: {explanation}\")\n",
    "    \n",
    "    return {\n",
    "        'window_correlations': correlations,\n",
    "        'window_positions': positions,\n",
    "        'early_mean': early_mean if len(early_corrs) > 0 else None,\n",
    "        'late_mean': late_mean if len(late_corrs) > 0 else None,\n",
    "        'trend_correlation': trend_corr if len(correlations) > 2 else None,\n",
    "        'trend_p_value': trend_p if len(correlations) > 2 else None,\n",
    "        'evaluation': evaluation,\n",
    "        'explanation': explanation,\n",
    "        'n_windows': len(correlations)\n",
    "    }\n",
    "\n",
    "# Run sliding window analysis\n",
    "if adata is not None:\n",
    "    sliding_results = sliding_window_correlation_analysis(adata)\n",
    "    \n",
    "    if sliding_results:\n",
    "        print(f\"\\n‚úÖ Sliding Window Analysis Complete: {sliding_results['evaluation']}\")\n",
    "    else:\n",
    "        print(\"‚ùå Sliding Window Analysis Failed\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping sliding window analysis - no data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Group 2: Sequential Failure of Proteostasis\n",
    "\n",
    "This section analyzes the systematic breakdown of protein quality control mechanisms.\n",
    "\n",
    "### Key Statement: Covariate-Controlled Differential Expression\n",
    "\n",
    "**Claim**: 36.14% of proteins (2,115/5,853) show significant alterations when controlling for age, PMI, and PatientID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariate_controlled_differential_expression(adata, max_proteins=None):\n",
    "    \"\"\"\n",
    "    Comprehensive covariate-controlled differential expression analysis\n",
    "    \n",
    "    This analysis removes confounding effects from age, PMI, and PatientID\n",
    "    to identify pure tau-related protein changes.\n",
    "    \"\"\"\n",
    "    if adata is None:\n",
    "        print(\"‚ùå No data available for analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üß™ Covariate-Controlled Differential Expression - Group 2, Statement 1\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Check required covariates\n",
    "    print(\"\\nüîç Step 1: Covariate Availability Check\")\n",
    "    \n",
    "    required_covariates = ['age', 'PMI', 'PatientID']\n",
    "    available_covariates = []\n",
    "    missing_covariates = []\n",
    "    \n",
    "    for cov in required_covariates:\n",
    "        # Try exact match first\n",
    "        if cov in adata.obs.columns:\n",
    "            available_covariates.append(cov)\n",
    "        else:\n",
    "            # Try case-insensitive search\n",
    "            matches = [c for c in adata.obs.columns if cov.lower() in c.lower()]\n",
    "            if matches:\n",
    "                print(f\"   üìù Using {matches[0]} for {cov}\")\n",
    "                adata.obs[cov] = adata.obs[matches[0]]  # Create standardized name\n",
    "                available_covariates.append(cov)\n",
    "            else:\n",
    "                missing_covariates.append(cov)\n",
    "    \n",
    "    print(f\"   ‚úÖ Available covariates: {available_covariates}\")\n",
    "    if missing_covariates:\n",
    "        print(f\"   ‚ö†Ô∏è Missing covariates: {missing_covariates}\")\n",
    "    \n",
    "    if not available_covariates:\n",
    "        print(\"‚ùå No covariates available - cannot perform covariate-controlled analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Data preparation and validation\n",
    "    print(\"\\nüìä Step 2: Data Preparation\")\n",
    "    \n",
    "    # Check tau status\n",
    "    if 'tau_status' not in adata.obs.columns:\n",
    "        print(\"‚ùå tau_status column not found\")\n",
    "        return None\n",
    "    \n",
    "    tau_counts = adata.obs['tau_status'].value_counts()\n",
    "    print(f\"   üß† Tau status distribution: {dict(tau_counts)}\")\n",
    "    \n",
    "    # Set analysis scope\n",
    "    if max_proteins is None:\n",
    "        max_proteins = min(adata.n_vars, 5853)  # Full analysis or dataset limit\n",
    "    else:\n",
    "        max_proteins = min(max_proteins, adata.n_vars)\n",
    "    \n",
    "    print(f\"   üî¨ Analyzing {max_proteins} proteins (out of {adata.n_vars} available)\")\n",
    "    \n",
    "    # Step 3: Linear model analysis\n",
    "    print(\"\\nüìà Step 3: Linear Model Analysis\")\n",
    "    print(\"   Running covariate-controlled differential expression...\")\n",
    "    \n",
    "    # Build formula\n",
    "    formula_parts = ['expression ~ tau_status']\n",
    "    for cov in available_covariates:\n",
    "        if cov == 'PatientID':\n",
    "            formula_parts.append('C(PatientID)')  # Categorical\n",
    "        else:\n",
    "            formula_parts.append(cov)  # Continuous\n",
    "    \n",
    "    formula = ' + '.join(formula_parts)\n",
    "    print(f\"   üìù Model formula: {formula}\")\n",
    "    \n",
    "    results = []\n",
    "    failed_proteins = []\n",
    "    \n",
    "    # Prepare observation dataframe\n",
    "    obs_df = adata.obs.copy()\n",
    "    obs_df['tau_status'] = pd.Categorical(obs_df['tau_status'])\n",
    "    \n",
    "    print(\"\\nüîÑ Processing proteins...\")\n",
    "    for i, protein in enumerate(tqdm(adata.var_names[:max_proteins], desc=\"Analyzing proteins\")):\n",
    "        try:\n",
    "            # Extract expression for this protein\n",
    "            expr = adata[:, protein].X.flatten()\n",
    "            obs_df['expression'] = expr\n",
    "            \n",
    "            # Fit linear model\n",
    "            model = ols(formula, data=obs_df)\n",
    "            results_fit = model.fit()\n",
    "            \n",
    "            # Extract tau coefficient\n",
    "            tau_coef = None\n",
    "            tau_pval = None\n",
    "            \n",
    "            # Look for tau coefficient (different parameterizations possible)\n",
    "            for param_name in results_fit.params.index:\n",
    "                if 'tau_status' in param_name and 'positive' in param_name:\n",
    "                    tau_coef = results_fit.params[param_name]\n",
    "                    tau_pval = results_fit.pvalues[param_name]\n",
    "                    break\n",
    "            \n",
    "            if tau_coef is None:\n",
    "                # Try alternative parameterization\n",
    "                for param_name in results_fit.params.index:\n",
    "                    if 'tau_status' in param_name:\n",
    "                        tau_coef = results_fit.params[param_name]\n",
    "                        tau_pval = results_fit.pvalues[param_name]\n",
    "                        break\n",
    "            \n",
    "            # Calculate simple fold change for comparison\n",
    "            tau_pos_mean = adata[adata.obs['tau_status'] == 'positive'][:, protein].X.mean()\n",
    "            tau_neg_mean = adata[adata.obs['tau_status'] == 'negative'][:, protein].X.mean()\n",
    "            simple_fc = tau_pos_mean - tau_neg_mean\n",
    "            \n",
    "            results.append({\n",
    "                'protein': protein,\n",
    "                'log2fc_adjusted': tau_coef if tau_coef is not None else np.nan,\n",
    "                'pvalue_adjusted': tau_pval if tau_pval is not None else 1.0,\n",
    "                'log2fc_simple': simple_fc,\n",
    "                'r_squared': results_fit.rsquared,\n",
    "                'aic': results_fit.aic\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_proteins.append(protein)\n",
    "            results.append({\n",
    "                'protein': protein,\n",
    "                'log2fc_adjusted': np.nan,\n",
    "                'pvalue_adjusted': 1.0,\n",
    "                'log2fc_simple': np.nan,\n",
    "                'r_squared': 0,\n",
    "                'aic': np.inf\n",
    "            })\n",
    "    \n",
    "    if failed_proteins:\n",
    "        print(f\"   ‚ö†Ô∏è Failed to analyze {len(failed_proteins)} proteins\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Step 4: Multiple testing correction\n",
    "    print(\"\\nüîß Step 4: Multiple Testing Correction (FDR)\")\n",
    "    \n",
    "    # Remove NaN p-values for FDR correction\n",
    "    valid_p = ~np.isnan(results_df['pvalue_adjusted'])\n",
    "    \n",
    "    if sum(valid_p) > 0:\n",
    "        rejected, fdr_corrected, _, _ = multipletests(\n",
    "            results_df.loc[valid_p, 'pvalue_adjusted'],\n",
    "            method='fdr_bh',\n",
    "            alpha=0.05\n",
    "        )\n",
    "        \n",
    "        # Initialize FDR column\n",
    "        results_df['fdr_adjusted'] = 1.0\n",
    "        results_df['significant_adjusted'] = False\n",
    "        \n",
    "        # Assign corrected values\n",
    "        results_df.loc[valid_p, 'fdr_adjusted'] = fdr_corrected\n",
    "        results_df.loc[valid_p, 'significant_adjusted'] = rejected\n",
    "        \n",
    "        # Results summary\n",
    "        n_total = len(results_df)\n",
    "        n_valid = sum(valid_p)\n",
    "        n_significant = sum(results_df['significant_adjusted'])\n",
    "        pct_significant = 100 * n_significant / n_total\n",
    "        \n",
    "        print(f\"   üìä Total proteins: {n_total}\")\n",
    "        print(f\"   ‚úÖ Valid analyses: {n_valid}\")\n",
    "        print(f\"   üî¨ Significant (FDR < 0.05): {n_significant}\")\n",
    "        print(f\"   üìà Percentage significant: {pct_significant:.2f}%\")\n",
    "        print(f\"   üéØ Expected: 36.14% (2,115/5,853)\")\n",
    "    \n",
    "    # Step 5: Results visualization\n",
    "    print(\"\\nüìä Step 5: Results Visualization\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Volcano plot\n",
    "    valid_results = results_df[valid_p]\n",
    "    if len(valid_results) > 0:\n",
    "        x = valid_results['log2fc_adjusted']\n",
    "        y = -np.log10(valid_results['pvalue_adjusted'])\n",
    "        colors = ['red' if sig else 'gray' for sig in valid_results['significant_adjusted']]\n",
    "        \n",
    "        axes[0, 0].scatter(x, y, c=colors, alpha=0.6, s=10)\n",
    "        axes[0, 0].axhline(y=-np.log10(0.05), color='black', linestyle='--', alpha=0.5)\n",
    "        axes[0, 0].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "        axes[0, 0].set_xlabel('Log2 Fold Change (Adjusted)')\n",
    "        axes[0, 0].set_ylabel('-Log10(P-value)')\n",
    "        axes[0, 0].set_title(f'Volcano Plot\\n({n_significant} significant)')\n",
    "    \n",
    "    # 2. FDR distribution\n",
    "    fdr_values = results_df['fdr_adjusted'][valid_p]\n",
    "    axes[0, 1].hist(fdr_values, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 1].axvline(x=0.05, color='red', linestyle='--', label='FDR = 0.05')\n",
    "    axes[0, 1].set_xlabel('FDR')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].set_title('FDR Distribution')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Effect size distribution\n",
    "    fc_sig = results_df[results_df['significant_adjusted']]['log2fc_adjusted']\n",
    "    fc_nonsig = results_df[~results_df['significant_adjusted']]['log2fc_adjusted']\n",
    "    \n",
    "    axes[0, 2].hist([fc_nonsig.dropna(), fc_sig.dropna()], bins=30, \n",
    "                   label=['Non-significant', 'Significant'],\n",
    "                   color=['gray', 'red'], alpha=0.7, edgecolor='black')\n",
    "    axes[0, 2].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[0, 2].set_xlabel('Log2 Fold Change')\n",
    "    axes[0, 2].set_ylabel('Count')\n",
    "    axes[0, 2].set_title('Effect Size Distribution')\n",
    "    axes[0, 2].legend()\n",
    "    \n",
    "    # 4. Simple vs adjusted fold changes\n",
    "    axes[1, 0].scatter(results_df['log2fc_simple'], results_df['log2fc_adjusted'], \n",
    "                      alpha=0.5, s=10)\n",
    "    lims = [-4, 4]\n",
    "    axes[1, 0].plot(lims, lims, 'r--', alpha=0.5)  # Identity line\n",
    "    axes[1, 0].set_xlabel('Simple Log2FC')\n",
    "    axes[1, 0].set_ylabel('Adjusted Log2FC')\n",
    "    axes[1, 0].set_title('Simple vs Adjusted Fold Changes')\n",
    "    axes[1, 0].set_xlim(lims)\n",
    "    axes[1, 0].set_ylim(lims)\n",
    "    \n",
    "    # 5. Model fit quality (R-squared)\n",
    "    r2_values = results_df['r_squared'][results_df['r_squared'] < 1]\n",
    "    axes[1, 1].hist(r2_values, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('R¬≤')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    axes[1, 1].set_title(f'Model Fit Quality\\nMean R¬≤ = {r2_values.mean():.3f}')\n",
    "    \n",
    "    # 6. Summary statistics\n",
    "    axes[1, 2].axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    COVARIATE-CONTROLLED DE SUMMARY\n",
    "    \n",
    "    üìä Analysis Scope:\n",
    "    Total proteins: {n_total:,}\n",
    "    Valid analyses: {n_valid:,}\n",
    "    \n",
    "    üî¨ Results:\n",
    "    Significant: {n_significant:,}\n",
    "    Percentage: {pct_significant:.2f}%\n",
    "    Expected: 36.14%\n",
    "    \n",
    "    üìà Covariates Used:\n",
    "    {', '.join(available_covariates)}\n",
    "    \n",
    "    üìè Effect Sizes:\n",
    "    Mean |FC|: {np.mean(np.abs(results_df['log2fc_adjusted'].dropna())):.3f}\n",
    "    \n",
    "    üéØ EVALUATION:\n",
    "    {'SUPPORTED' if abs(pct_significant - 36.14) < 5 else 'NEEDS REVIEW'}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 2].text(0.1, 0.5, summary_text, fontsize=10, verticalalignment='center',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\", alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Step 6: Evaluation\n",
    "    print(\"\\nüéØ Step 6: Statement Evaluation\")\n",
    "    \n",
    "    # Evaluation criteria\n",
    "    expected_percentage = 36.14\n",
    "    percentage_diff = abs(pct_significant - expected_percentage)\n",
    "    \n",
    "    if percentage_diff < 5:  # Within 5% tolerance\n",
    "        evaluation = \"SUPPORTED\"\n",
    "        explanation = f\"Found {pct_significant:.2f}% significant proteins, closely matching expected 36.14%\"\n",
    "    elif percentage_diff < 10:  # Within 10% tolerance\n",
    "        evaluation = \"PARTIALLY SUPPORTED\"\n",
    "        explanation = f\"Found {pct_significant:.2f}% significant proteins, somewhat different from expected 36.14%\"\n",
    "    else:\n",
    "        evaluation = \"REFUTED\"\n",
    "        explanation = f\"Found {pct_significant:.2f}% significant proteins, substantially different from expected 36.14%\"\n",
    "    \n",
    "    print(f\"   üìã Evaluation: {evaluation}\")\n",
    "    print(f\"   üìù Explanation: {explanation}\")\n",
    "    \n",
    "    return {\n",
    "        'results_df': results_df,\n",
    "        'n_total': n_total,\n",
    "        'n_significant': n_significant,\n",
    "        'percentage_significant': pct_significant,\n",
    "        'expected_percentage': expected_percentage,\n",
    "        'available_covariates': available_covariates,\n",
    "        'evaluation': evaluation,\n",
    "        'explanation': explanation\n",
    "    }\n",
    "\n",
    "# Run covariate-controlled analysis\n",
    "if adata is not None:\n",
    "    # For demonstration, analyze first 500 proteins (set to None for full analysis)\n",
    "    de_results = covariate_controlled_differential_expression(adata, max_proteins=500)\n",
    "    \n",
    "    if de_results:\n",
    "        print(f\"\\n‚úÖ Covariate-Controlled DE Complete: {de_results['evaluation']}\")\n",
    "        print(f\"üìä Analysis summary: {de_results['n_significant']}/{de_results['n_total']} proteins significant\")\n",
    "    else:\n",
    "        print(\"‚ùå Covariate-Controlled DE Analysis Failed\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping covariate-controlled DE analysis - no data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ AI Automation Demonstration\n",
    "\n",
    "This section demonstrates the AI automation capabilities for large-scale analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AI automation modules\n",
    "try:\n",
    "    import sys\n",
    "    sys.path.append('ai_automation')\n",
    "    from ai_agent import BioinformaticsAgent, EvaluationResult\n",
    "    from analysis_automation import AnalysisAutomation\n",
    "    \n",
    "    print(\"‚úÖ AI automation modules imported successfully\")\n",
    "    \n",
    "    if adata is not None:\n",
    "        print(\"\\nü§ñ Initializing AI Automation...\")\n",
    "        \n",
    "        # Initialize automation\n",
    "        automation = AnalysisAutomation(adata)\n",
    "        \n",
    "        # Demonstrate automated correlation analysis\n",
    "        print(\"\\nüìä Running automated SQSTM1 correlation analysis...\")\n",
    "        proteins = ['TAX1BP1', 'CAT', 'VDAC1', 'CYCS', 'ATP5F1A']\n",
    "        \n",
    "        try:\n",
    "            sqstm1_corr = automation.calculate_protein_correlations(proteins, 'SQSTM1')\n",
    "            if not sqstm1_corr.empty:\n",
    "                print(\"\\nüß¨ SQSTM1 Correlation Results:\")\n",
    "                for _, row in sqstm1_corr.iterrows():\n",
    "                    print(f\"   {row['protein']}: r={row['correlation']:.3f}, p={row['p_value']:.3e}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No correlations computed\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Automated correlation analysis failed: {str(e)}\")\n",
    "        \n",
    "        # Demonstrate sliding window automation\n",
    "        print(\"\\nüìà Running automated sliding window analysis...\")\n",
    "        try:\n",
    "            sliding = automation.sliding_window_correlation('SQSTM1', 'VDAC1')\n",
    "            if not sliding.empty:\n",
    "                print(f\"   ‚úÖ Generated {len(sliding)} window correlations\")\n",
    "                if hasattr(sliding, 'attrs') and 'summary' in sliding.attrs:\n",
    "                    summary = sliding.attrs['summary']\n",
    "                    print(f\"   üìä Early mean: {summary.get('early_mean_correlation', 'N/A'):.3f}\")\n",
    "                    print(f\"   üìä Late mean: {summary.get('late_mean_correlation', 'N/A'):.3f}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No sliding window results generated\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Automated sliding window analysis failed: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ AI automation demonstration complete\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Cannot run AI automation - no data loaded\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è AI automation modules not available: {str(e)}\")\n",
    "    print(\"üìù This is expected if running without the full project structure\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è AI automation error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Comprehensive Results Summary\n",
    "\n",
    "This section provides a comprehensive summary of all analyses performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_summary():\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary of all analysis results\n",
    "    \"\"\"\n",
    "    print(\"üìã COMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Data loading summary\n",
    "    if adata is not None:\n",
    "        print(f\"\\nüìä Dataset Summary:\")\n",
    "        print(f\"   üì¶ Dimensions: {adata.n_obs} cells √ó {adata.n_vars} proteins\")\n",
    "        if 'tau_status' in adata.obs.columns:\n",
    "            tau_counts = adata.obs['tau_status'].value_counts()\n",
    "            print(f\"   üß† Tau distribution: {dict(tau_counts)}\")\n",
    "        print(f\"   ‚úÖ Data successfully loaded and validated\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Dataset: Not loaded\")\n",
    "    \n",
    "    # Group 1 results\n",
    "    print(f\"\\nüß¨ Group 1: Late-Stage Mitochondrial Dysregulation\")\n",
    "    print(f\"   {'='*50}\")\n",
    "    \n",
    "    # UPS analysis results\n",
    "    if 'ups_results' in globals() and ups_results:\n",
    "        print(f\"   üìã Statement 1 (UPS Proteins): {ups_results['evaluation']}\")\n",
    "        print(f\"      üìä Analyzed: {ups_results['n_total']} proteins\")\n",
    "        print(f\"      üìà Significant: {ups_results['pct_significant']:.1f}%\")\n",
    "        print(f\"      üìè Mean effect size: {ups_results['mean_effect_size']:.3f}\")\n",
    "    else:\n",
    "        print(f\"   üìã Statement 1 (UPS Proteins): Not analyzed\")\n",
    "    \n",
    "    # SQSTM1 analysis results\n",
    "    if 'sqstm1_results' in globals() and sqstm1_results:\n",
    "        print(f\"   üìã Statement 2 (SQSTM1): {sqstm1_results['evaluation']}\")\n",
    "        print(f\"      üìä Log2FC: {sqstm1_results['log2fc_observed']:.3f} (expected: {sqstm1_results['log2fc_expected']})\")\n",
    "        print(f\"      üìà Fold change: {sqstm1_results['fold_change']:.1f}x\")\n",
    "        print(f\"      üé≤ P-value: {sqstm1_results['p_value']:.2e}\")\n",
    "    else:\n",
    "        print(f\"   üìã Statement 2 (SQSTM1): Not analyzed\")\n",
    "    \n",
    "    # Sliding window results\n",
    "    if 'sliding_results' in globals() and sliding_results:\n",
    "        print(f\"   üìã Statement 6 (Sliding Window): {sliding_results['evaluation']}\")\n",
    "        print(f\"      üìä Windows analyzed: {sliding_results['n_windows']}\")\n",
    "        if sliding_results['early_mean'] is not None:\n",
    "            print(f\"      üåÖ Early correlation: {sliding_results['early_mean']:.3f} (expected: -0.417)\")\n",
    "        if sliding_results['late_mean'] is not None:\n",
    "            print(f\"      üåá Late correlation: {sliding_results['late_mean']:.3f} (expected: 0.478)\")\n",
    "        if sliding_results['trend_correlation'] is not None:\n",
    "            print(f\"      üìà Trend: r={sliding_results['trend_correlation']:.3f} (expected: 0.851)\")\n",
    "    else:\n",
    "        print(f\"   üìã Statement 6 (Sliding Window): Not analyzed\")\n",
    "    \n",
    "    # Group 2 results\n",
    "    print(f\"\\nüî¨ Group 2: Sequential Failure of Proteostasis\")\n",
    "    print(f\"   {'='*50}\")\n",
    "    \n",
    "    # Differential expression results\n",
    "    if 'de_results' in globals() and de_results:\n",
    "        print(f\"   üìã Statement 1 (Covariate-controlled DE): {de_results['evaluation']}\")\n",
    "        print(f\"      üìä Proteins analyzed: {de_results['n_total']:,}\")\n",
    "        print(f\"      üìà Significant: {de_results['n_significant']:,} ({de_results['percentage_significant']:.2f}%)\")\n",
    "        print(f\"      üéØ Expected: {de_results['expected_percentage']:.2f}%\")\n",
    "        print(f\"      üîß Covariates: {', '.join(de_results['available_covariates'])}\")\n",
    "    else:\n",
    "        print(f\"   üìã Statement 1 (Covariate-controlled DE): Not analyzed\")\n",
    "    \n",
    "    # Overall evaluation\n",
    "    print(f\"\\nüéØ Overall Project Evaluation\")\n",
    "    print(f\"   {'='*40}\")\n",
    "    \n",
    "    # Count evaluations\n",
    "    evaluations = []\n",
    "    if 'ups_results' in globals() and ups_results:\n",
    "        evaluations.append(ups_results['evaluation'])\n",
    "    if 'sqstm1_results' in globals() and sqstm1_results:\n",
    "        evaluations.append(sqstm1_results['evaluation'])\n",
    "    if 'sliding_results' in globals() and sliding_results:\n",
    "        evaluations.append(sliding_results['evaluation'])\n",
    "    if 'de_results' in globals() and de_results:\n",
    "        evaluations.append(de_results['evaluation'])\n",
    "    \n",
    "    if evaluations:\n",
    "        supported = sum(1 for e in evaluations if e == 'SUPPORTED')\n",
    "        partially = sum(1 for e in evaluations if e == 'PARTIALLY SUPPORTED')\n",
    "        refuted = sum(1 for e in evaluations if e == 'REFUTED')\n",
    "        total = len(evaluations)\n",
    "        \n",
    "        print(f\"   üìä Statements analyzed: {total}\")\n",
    "        print(f\"   ‚úÖ Supported: {supported}\")\n",
    "        print(f\"   üî∂ Partially supported: {partially}\")\n",
    "        print(f\"   ‚ùå Refuted: {refuted}\")\n",
    "        print(f\"   üìà Success rate: {100*supported/total:.1f}%\")\n",
    "    else:\n",
    "        print(f\"   üìä No analyses completed\")\n",
    "    \n",
    "    # Technical summary\n",
    "    print(f\"\\nüîß Technical Implementation Summary\")\n",
    "    print(f\"   {'='*40}\")\n",
    "    print(f\"   üìã Framework: Comprehensive bioinformatics evaluation system\")\n",
    "    print(f\"   üìä Statistical methods: FDR correction, bootstrap CI, linear models\")\n",
    "    print(f\"   üî¨ Advanced analyses: Sliding window, covariate control, temporal analysis\")\n",
    "    print(f\"   ü§ñ AI automation: Integrated automated analysis capabilities\")\n",
    "    print(f\"   üìà Visualization: Publication-quality plots and comprehensive summaries\")\n",
    "    print(f\"   üìù Documentation: Enhanced analytical rationale and biological context\")\n",
    "    \n",
    "    print(f\"\\nüéâ Analysis Framework Successfully Demonstrated!\")\n",
    "    print(f\"\\nüìö For more details, see individual analysis sections above.\")\n",
    "\n",
    "# Generate the comprehensive summary\n",
    "generate_comprehensive_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps and Local Execution Guide\n",
    "\n",
    "### To run this notebook locally:\n",
    "\n",
    "1. **Install Dependencies**:\n",
    "   ```bash\n",
    "   pip install pandas numpy scipy scanpy statsmodels scikit-learn matplotlib seaborn tqdm\n",
    "   ```\n",
    "\n",
    "2. **Data Requirements**:\n",
    "   - Ensure `data/pool_processed_v2.h5ad` file is present\n",
    "   - Data should contain required metadata columns: `tau_status`, `MC1`, `pseudotime`\n",
    "\n",
    "3. **Running Individual Analyses**:\n",
    "   - Each analysis can be run independently\n",
    "   - Modify `max_proteins` parameter for faster testing\n",
    "   - Set to `None` for full analysis\n",
    "\n",
    "4. **Customization Options**:\n",
    "   - Adjust statistical thresholds in evaluation functions\n",
    "   - Modify visualization parameters\n",
    "   - Add additional protein lists or analysis methods\n",
    "\n",
    "5. **Performance Considerations**:\n",
    "   - Full analysis of 5,853 proteins may take 10-30 minutes\n",
    "   - Use subset analysis for testing (`max_proteins=500`)\n",
    "   - Results are cached where possible\n",
    "\n",
    "### Advanced Features:\n",
    "- **AI Automation**: Automated analysis pipelines\n",
    "- **Statistical Rigor**: Multiple testing correction, effect sizes\n",
    "- **Temporal Analysis**: Sliding window correlations\n",
    "- **Covariate Control**: Advanced linear modeling\n",
    "- **Publication Quality**: Professional visualizations\n",
    "\n",
    "This framework provides a complete solution for rigorous evaluation of biological statements using proteomic data, with comprehensive documentation and statistical validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}