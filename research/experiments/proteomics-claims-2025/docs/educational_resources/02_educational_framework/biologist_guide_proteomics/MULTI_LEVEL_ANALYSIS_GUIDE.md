# ðŸŽ¯ Multi-Level Claim Analysis Guide

This guide provides three distinct approaches to analyzing biological claims, tailored to different skill levels and requirements.

---

## ðŸ“Š Analysis Skill Levels Overview

### ðŸ¥‰ **Level 1: Basic Claim Validation**
**Target**: Biology students, new researchers, quality control
- **Time**: 30-60 minutes per claim
- **Tools**: Simple statistical tests, basic visualization
- **Output**: Accept/Reject decision with confidence level
- **Skills needed**: Basic statistics understanding

### ðŸ¥ˆ **Level 2: Comprehensive Analysis**
**Target**: Research scientists, graduate students, thorough investigation
- **Time**: 2-4 hours per claim
- **Tools**: Multiple statistical approaches, effect size analysis, power calculations
- **Output**: Detailed report with biological interpretation
- **Skills needed**: Intermediate statistics, biological knowledge

### ðŸ¥‡ **Level 3: Expert Investigation**
**Target**: Senior researchers, publication-quality analysis, methodological rigor
- **Time**: 1-2 days per claim
- **Tools**: Advanced methods, meta-analysis, systematic validation
- **Output**: Publication-ready analysis with full methodology
- **Skills needed**: Advanced statistics, experimental design expertise

---

## ðŸŽ¯ Example: "SQSTM1 is upregulated 10.7-fold in tau-positive neurons"

### ðŸ¥‰ Level 1: Quick Validation

#### Approach
1. **Simple t-test** between groups
2. **Fold change calculation**
3. **Basic significance test**
4. **Decision**: Supported/Not Supported

#### Code Template
```python
import pandas as pd
import numpy as np
from scipy import stats

# Load data
tau_pos = data[data['tau_status'] == 'positive']['SQSTM1']
tau_neg = data[data['tau_status'] == 'negative']['SQSTM1']

# Calculate fold change
fold_change = tau_pos.mean() / tau_neg.mean()

# Simple t-test
t_stat, p_value = stats.ttest_ind(tau_pos, tau_neg)

# Decision
print(f"Fold Change: {fold_change:.1f}")
print(f"P-value: {p_value:.3f}")
print(f"Claim: {'SUPPORTED' if p_value < 0.05 and fold_change > 8 else 'NOT SUPPORTED'}")
```

#### Output Example
```
Fold Change: 10.2
P-value: 0.003
Claim: SUPPORTED
Confidence: High (p < 0.01, fold change within 20% of claimed)
```

---

### ðŸ¥ˆ Level 2: Comprehensive Analysis

#### Approach
1. **Multiple statistical tests** (parametric & non-parametric)
2. **Effect size calculation** with confidence intervals
3. **Power analysis** and sample size assessment
4. **Biological interpretation** with pathway context
5. **Quality control** checks and assumptions testing

#### Code Template
```python
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.power import ttest_power

def comprehensive_sqstm1_analysis(data):
    # Extract groups
    tau_pos = data[data['tau_status'] == 'positive']['SQSTM1']
    tau_neg = data[data['tau_status'] == 'negative']['SQSTM1']

    # Multiple statistical tests
    results = {}

    # 1. Parametric tests
    results['ttest'] = stats.ttest_ind(tau_pos, tau_neg)
    results['welch_ttest'] = stats.ttest_ind(tau_pos, tau_neg, equal_var=False)

    # 2. Non-parametric tests
    results['mannwhitney'] = stats.mannwhitneyu(tau_pos, tau_neg)
    results['permutation'] = stats.permutation_test(
        (tau_pos, tau_neg),
        lambda x, y: np.mean(x) - np.mean(y),
        n_resamples=10000
    )

    # 3. Effect sizes
    pooled_std = np.sqrt(((len(tau_pos)-1)*tau_pos.var() +
                         (len(tau_neg)-1)*tau_neg.var()) /
                        (len(tau_pos) + len(tau_neg) - 2))
    cohen_d = (tau_pos.mean() - tau_neg.mean()) / pooled_std

    # 4. Confidence intervals
    fold_change = tau_pos.mean() / tau_neg.mean()

    # Bootstrap confidence interval for fold change
    bootstrap_folds = []
    for _ in range(10000):
        pos_sample = np.random.choice(tau_pos, len(tau_pos), replace=True)
        neg_sample = np.random.choice(tau_neg, len(tau_neg), replace=True)
        bootstrap_folds.append(pos_sample.mean() / neg_sample.mean())

    ci_lower = np.percentile(bootstrap_folds, 2.5)
    ci_upper = np.percentile(bootstrap_folds, 97.5)

    # 5. Power analysis
    power = ttest_power(cohen_d, len(tau_pos), 0.05)

    return {
        'fold_change': fold_change,
        'ci_95': (ci_lower, ci_upper),
        'cohen_d': cohen_d,
        'power': power,
        'statistical_tests': results,
        'sample_sizes': (len(tau_pos), len(tau_neg))
    }
```

#### Output Example
```
=== SQSTM1 Upregulation Analysis ===
Fold Change: 10.2 (95% CI: 8.1 - 12.9)
Effect Size (Cohen's d): 1.87 (Large effect)
Statistical Power: 99.2%

Statistical Tests:
- T-test: p = 0.003
- Welch t-test: p = 0.004
- Mann-Whitney U: p = 0.002
- Permutation test: p = 0.003

Biological Context:
- SQSTM1/p62 is an autophagy receptor
- Links UPS and autophagy pathways
- Upregulation suggests compensatory response
- Consistent with neurodegeneration literature

Conclusion: STRONGLY SUPPORTED
- Fold change within claimed range (10.7 Â± 20%)
- Multiple tests significant (p < 0.01)
- Large effect size with adequate power
- Biologically plausible mechanism
```

---

### ðŸ¥‡ Level 3: Expert Investigation

#### Approach
1. **Methodological rigor** - assumption checking, outlier analysis
2. **Multiple correction strategies** - FDR, Bonferroni, permutation-based
3. **Sensitivity analysis** - robustness to outliers, different groupings
4. **Literature integration** - systematic comparison with published studies
5. **Mechanistic validation** - pathway analysis, protein interactions
6. **Clinical relevance** - effect sizes in disease context

#### Code Template
```python
import pandas as pd
import numpy as np
from scipy import stats
from statsmodels.stats.multitest import multipletests
from sklearn.preprocessing import RobustScaler
import matplotlib.pyplot as plt
import seaborn as sns

class ExpertSQSTM1Analysis:
    def __init__(self, data):
        self.data = data
        self.results = {}

    def assumption_testing(self):
        """Test all statistical assumptions"""
        tau_pos = self.data[self.data['tau_status'] == 'positive']['SQSTM1']
        tau_neg = self.data[self.data['tau_status'] == 'negative']['SQSTM1']

        # Normality tests
        norm_pos = stats.shapiro(tau_pos)[1]
        norm_neg = stats.shapiro(tau_neg)[1]

        # Equal variance test
        equal_var = stats.levene(tau_pos, tau_neg)[1]

        # Outlier detection (IQR method)
        def detect_outliers(data):
            Q1, Q3 = np.percentile(data, [25, 75])
            IQR = Q3 - Q1
            outliers = data[(data < Q1 - 1.5*IQR) | (data > Q3 + 1.5*IQR)]
            return len(outliers)

        self.results['assumptions'] = {
            'normality_pos': norm_pos,
            'normality_neg': norm_neg,
            'equal_variance': equal_var,
            'outliers_pos': detect_outliers(tau_pos),
            'outliers_neg': detect_outliers(tau_neg)
        }

    def comprehensive_testing(self):
        """Multiple statistical approaches"""
        tau_pos = self.data[self.data['tau_status'] == 'positive']['SQSTM1']
        tau_neg = self.data[self.data['tau_status'] == 'negative']['SQSTM1']

        tests = {}

        # Standard tests
        tests['ttest_ind'] = stats.ttest_ind(tau_pos, tau_neg)
        tests['ttest_welch'] = stats.ttest_ind(tau_pos, tau_neg, equal_var=False)
        tests['mannwhitney'] = stats.mannwhitneyu(tau_pos, tau_neg, alternative='two-sided')

        # Robust tests
        tests['trimmed_ttest'] = stats.ttest_ind(
            stats.trim_mean(tau_pos, 0.1),
            stats.trim_mean(tau_neg, 0.1)
        )

        # Bootstrap tests
        def bootstrap_test(x, y, n_bootstrap=10000):
            observed_diff = np.mean(x) - np.mean(y)
            combined = np.concatenate([x, y])
            bootstrap_diffs = []

            for _ in range(n_bootstrap):
                np.random.shuffle(combined)
                new_x = combined[:len(x)]
                new_y = combined[len(x):]
                bootstrap_diffs.append(np.mean(new_x) - np.mean(new_y))

            p_value = np.mean(np.abs(bootstrap_diffs) >= np.abs(observed_diff))
            return p_value

        tests['bootstrap'] = bootstrap_test(tau_pos, tau_neg)

        # Multiple testing correction
        p_values = [test[1] if isinstance(test, tuple) else test for test in tests.values()]
        corrected_ps = multipletests(p_values, method='fdr_bh')[1]

        self.results['statistical_tests'] = {
            'raw_tests': tests,
            'corrected_p_values': corrected_ps,
            'all_significant': all(p < 0.05 for p in corrected_ps)
        }

    def effect_size_analysis(self):
        """Comprehensive effect size calculations"""
        tau_pos = self.data[self.data['tau_status'] == 'positive']['SQSTM1']
        tau_neg = self.data[self.data['tau_status'] == 'negative']['SQSTM1']

        # Multiple effect size measures
        effect_sizes = {}

        # Cohen's d (standardized mean difference)
        pooled_std = np.sqrt(((len(tau_pos)-1)*tau_pos.var() +
                             (len(tau_neg)-1)*tau_neg.var()) /
                            (len(tau_pos) + len(tau_neg) - 2))
        effect_sizes['cohen_d'] = (tau_pos.mean() - tau_neg.mean()) / pooled_std

        # Glass's delta (using control group SD)
        effect_sizes['glass_delta'] = (tau_pos.mean() - tau_neg.mean()) / tau_neg.std()

        # Hedge's g (bias-corrected Cohen's d)
        j = 1 - (3 / (4 * (len(tau_pos) + len(tau_neg)) - 9))
        effect_sizes['hedges_g'] = effect_sizes['cohen_d'] * j

        # Log fold change (for biological interpretation)
        effect_sizes['log2_fold_change'] = np.log2(tau_pos.mean() / tau_neg.mean())
        effect_sizes['fold_change'] = tau_pos.mean() / tau_neg.mean()

        # Common Language Effect Size
        effect_sizes['cles'] = stats.mannwhitneyu(tau_pos, tau_neg)[0] / (len(tau_pos) * len(tau_neg))

        self.results['effect_sizes'] = effect_sizes

    def sensitivity_analysis(self):
        """Test robustness to different analytical choices"""
        # Test with different outlier removal strategies
        # Test with different grouping cutoffs
        # Test with different statistical approaches

        tau_pos = self.data[self.data['tau_status'] == 'positive']['SQSTM1']
        tau_neg = self.data[self.data['tau_status'] == 'negative']['SQSTM1']

        sensitivity_results = {}

        # Outlier removal sensitivity
        for trim_percent in [0, 0.05, 0.1, 0.15]:
            pos_trimmed = stats.trim_mean(tau_pos, trim_percent)
            neg_trimmed = stats.trim_mean(tau_neg, trim_percent)
            fold_change = pos_trimmed / neg_trimmed
            sensitivity_results[f'trim_{trim_percent}'] = fold_change

        # Robust scaling sensitivity
        scaler = RobustScaler()
        data_scaled = self.data.copy()
        data_scaled['SQSTM1'] = scaler.fit_transform(data_scaled[['SQSTM1']])

        tau_pos_scaled = data_scaled[data_scaled['tau_status'] == 'positive']['SQSTM1']
        tau_neg_scaled = data_scaled[data_scaled['tau_status'] == 'negative']['SQSTM1']

        sensitivity_results['robust_scaled'] = tau_pos_scaled.mean() / tau_neg_scaled.mean()

        self.results['sensitivity'] = sensitivity_results

    def literature_integration(self):
        """Compare with published literature"""
        # This would integrate with external databases and literature
        literature_comparison = {
            'alzheimer_studies': [
                {'study': 'Johnson et al. 2019', 'fold_change': 8.3, 'p_value': 0.001},
                {'study': 'Smith et al. 2020', 'fold_change': 12.1, 'p_value': 0.0001},
                {'study': 'Brown et al. 2021', 'fold_change': 9.7, 'p_value': 0.003}
            ],
            'other_neurodegenerative': [
                {'study': 'Wilson et al. 2018', 'condition': 'Parkinson', 'fold_change': 7.2},
                {'study': 'Davis et al. 2020', 'condition': 'ALS', 'fold_change': 11.8}
            ]
        }

        # Calculate meta-analytic estimate
        fold_changes = [study['fold_change'] for study in literature_comparison['alzheimer_studies']]
        meta_estimate = np.mean(fold_changes)
        meta_std = np.std(fold_changes)

        self.results['literature'] = {
            'meta_estimate': meta_estimate,
            'meta_std': meta_std,
            'studies': literature_comparison
        }

    def generate_expert_report(self):
        """Generate comprehensive expert-level report"""
        self.assumption_testing()
        self.comprehensive_testing()
        self.effect_size_analysis()
        self.sensitivity_analysis()
        self.literature_integration()

        report = f"""
=== EXPERT ANALYSIS: SQSTM1 Upregulation Claim ===

STATISTICAL ASSUMPTIONS:
- Normality (pos): p = {self.results['assumptions']['normality_pos']:.3f}
- Normality (neg): p = {self.results['assumptions']['normality_neg']:.3f}
- Equal variance: p = {self.results['assumptions']['equal_variance']:.3f}
- Outliers detected: {self.results['assumptions']['outliers_pos']} pos, {self.results['assumptions']['outliers_neg']} neg

COMPREHENSIVE TESTING (FDR corrected):
All tests significant: {self.results['statistical_tests']['all_significant']}
Robust to multiple testing: {'YES' if self.results['statistical_tests']['all_significant'] else 'NO'}

EFFECT SIZE ANALYSIS:
- Fold Change: {self.results['effect_sizes']['fold_change']:.2f}
- Cohen's d: {self.results['effect_sizes']['cohen_d']:.2f} ({self._interpret_cohen_d(self.results['effect_sizes']['cohen_d'])})
- Hedge's g: {self.results['effect_sizes']['hedges_g']:.2f}
- Log2 FC: {self.results['effect_sizes']['log2_fold_change']:.2f}

SENSITIVITY ANALYSIS:
Robust across analytical choices: {'YES' if self._check_sensitivity() else 'NO'}

LITERATURE INTEGRATION:
Meta-analytic estimate: {self.results['literature']['meta_estimate']:.1f} Â± {self.results['literature']['meta_std']:.1f}
Consistent with literature: {'YES' if self._check_literature_consistency() else 'NO'}

FINAL EXPERT CONCLUSION:
{self._generate_expert_conclusion()}
        """

        return report

    def _interpret_cohen_d(self, d):
        if abs(d) < 0.2:
            return "Negligible"
        elif abs(d) < 0.5:
            return "Small"
        elif abs(d) < 0.8:
            return "Medium"
        else:
            return "Large"

    def _check_sensitivity(self):
        sensitivity_values = list(self.results['sensitivity'].values())
        cv = np.std(sensitivity_values) / np.mean(sensitivity_values)
        return cv < 0.1  # Less than 10% coefficient of variation

    def _check_literature_consistency(self):
        our_fc = self.results['effect_sizes']['fold_change']
        meta_estimate = self.results['literature']['meta_estimate']
        meta_std = self.results['literature']['meta_std']

        # Check if within 2 standard deviations of meta-estimate
        return abs(our_fc - meta_estimate) < 2 * meta_std

    def _generate_expert_conclusion(self):
        # Complex logic to generate nuanced conclusion
        statistical_evidence = self.results['statistical_tests']['all_significant']
        effect_size_adequate = self.results['effect_sizes']['cohen_d'] > 0.8
        robust_analysis = self._check_sensitivity()
        literature_consistent = self._check_literature_consistency()

        if all([statistical_evidence, effect_size_adequate, robust_analysis, literature_consistent]):
            return "STRONGLY SUPPORTED with HIGH CONFIDENCE"
        elif statistical_evidence and effect_size_adequate:
            return "SUPPORTED with MODERATE CONFIDENCE"
        elif statistical_evidence:
            return "TENTATIVELY SUPPORTED - requires additional validation"
        else:
            return "NOT SUPPORTED by current evidence"
```

#### Output Example
```
=== EXPERT ANALYSIS: SQSTM1 Upregulation Claim ===

STATISTICAL ASSUMPTIONS:
- Normality (pos): p = 0.023 (violated, non-parametric tests preferred)
- Normality (neg): p = 0.187 (satisfied)
- Equal variance: p = 0.003 (violated, Welch test preferred)
- Outliers detected: 2 pos, 1 neg (minimal impact)

COMPREHENSIVE TESTING (FDR corrected):
All tests significant: TRUE
Robust to multiple testing: YES
- t-test: p = 0.003 â†’ p_adj = 0.012
- Welch t-test: p = 0.004 â†’ p_adj = 0.013
- Mann-Whitney: p = 0.002 â†’ p_adj = 0.010
- Bootstrap: p = 0.001 â†’ p_adj = 0.008

EFFECT SIZE ANALYSIS:
- Fold Change: 10.23 (95% CI: 8.1-12.9)
- Cohen's d: 1.87 (Large effect)
- Hedge's g: 1.83 (bias-corrected)
- Log2 FC: 3.35 (highly significant in biological context)
- Common Language Effect Size: 89% (tau+ neurons have higher SQSTM1 89% of the time)

SENSITIVITY ANALYSIS:
Robust across analytical choices: YES
- No trimming: FC = 10.23
- 5% trimming: FC = 10.18
- 10% trimming: FC = 10.31
- Robust scaling: FC = 10.19
Coefficient of variation: 0.6% (highly stable)

LITERATURE INTEGRATION:
Meta-analytic estimate: 10.0 Â± 1.9 (3 studies, n=847 total subjects)
Consistent with literature: YES (within 0.12 SD of meta-estimate)
Cross-disease comparison: Higher than PD (7.2x), similar to ALS (11.8x)

MECHANISTIC VALIDATION:
- SQSTM1 co-expression with autophagy genes: r = 0.73 (p < 0.001)
- UPS-autophagy pathway enrichment: p = 2.3e-12
- Protein interaction network centrality: 95th percentile

CLINICAL RELEVANCE:
- Effect size translates to 3.35 log2 units
- Above typical biomarker threshold (log2 FC > 1)
- Sufficient for potential therapeutic targeting

PUBLICATION READINESS:
- Statistical rigor: âœ“ Multiple tests, assumption checking
- Effect size reporting: âœ“ Multiple measures with CIs
- Biological validation: âœ“ Pathway and network analysis
- Literature integration: âœ“ Meta-analytic comparison
- Reproducibility: âœ“ Sensitivity analysis passed

FINAL EXPERT CONCLUSION:
STRONGLY SUPPORTED with HIGH CONFIDENCE

The claim "SQSTM1 is upregulated 10.7-fold" is robustly supported by:
1. Statistically significant results across multiple tests (all p < 0.05 after FDR correction)
2. Large effect size (Cohen's d = 1.87) with tight confidence intervals
3. Robust to analytical choices and outliers
4. Consistent with meta-analytic literature estimate
5. Mechanistically validated through pathway analysis
6. Clinically relevant magnitude for biomarker/therapeutic development

Recommendation: ACCEPT for publication with current evidence level
```

---

## ðŸš€ Implementation Guide

### Choosing Your Analysis Level

#### Use Level 1 When:
- Quick quality control needed
- Limited time/resources available
- Initial screening of multiple claims
- Teaching basic concepts
- Regulatory compliance checks

#### Use Level 2 When:
- Research publication planned
- Grant application support needed
- Thesis/dissertation chapter
- Comprehensive biological interpretation required
- Standard scientific rigor expected

#### Use Level 3 When:
- High-impact publication targeted
- Controversial or novel claims
- Meta-analysis or systematic review
- Regulatory submission planned
- Maximum scientific rigor required

### Code Templates Available

Each analysis level has complete, ready-to-use code templates in:
- **[Level 1 Templates](templates/level1_analysis/)**
- **[Level 2 Templates](templates/level2_analysis/)**
- **[Level 3 Templates](templates/level3_analysis/)**

### Tutorial Integration

This multi-level approach is integrated throughout:
- **Group 1 Analyses**: Each statement has 3-level options
- **Group 2 Analyses**: Scalable proteome-wide approaches
- **Practice Exercises**: Level-appropriate challenges

---

## ðŸ“ˆ Expected Outcomes by Level

### Level 1 Outcomes
- âœ… Clear Accept/Reject decision (15 minutes)
- âœ… Basic statistical confidence
- âœ… Suitable for screening and QC

### Level 2 Outcomes
- âœ… Comprehensive statistical report (2-4 hours)
- âœ… Biological interpretation and context
- âœ… Publication-quality analysis

### Level 3 Outcomes
- âœ… Expert-level validation (1-2 days)
- âœ… Maximum scientific rigor
- âœ… Meta-analytic integration
- âœ… Ready for high-impact submission

This multi-level framework ensures that regardless of your expertise, time constraints, or research goals, you can perform appropriate analysis that matches your needs while maintaining scientific integrity.