# NegSl-AIS Technical Specification for IMMUNOS Implementation

## Executive Summary

This document extracts all technical details from the research paper "Negative Selection-based Artificial Immune System (NegSl-AIS) - A Hybrid Multimodal Emotional Effect Classification Model" for implementation in the IMMUNOS security dashboard.

**Key Results:**
- Arousal Classification: 96.48%
- Valence Classification: 98.63%
- Overall Accuracy: 94%
- Cohen's Kappa: 0.919
- Matthews Correlation Coefficient (MCC): 0.920

---

## 1. BIOLOGICAL IMMUNE SYSTEM ANALOGY

### 1.1 Core Concept: Negative Selection

The biological immune system distinguishes between "self" (body's own cells) and "non-self" (foreign pathogens) through a process called **negative selection** in the thymus.

**T-Cell Maturation Process (Figure 7):**

```
STEP I:   Samples in a feature vector of a particular class are called 
          "self samples" (analogous to self antigens)

STEP II:  Naïve T-Cells are the detectors generated by the model
          (random vectors in feature space)

STEP III: Detectors that bind to self-samples (self antigens) are 
          DISCARDED as invalid - "Discard Matching Detectors"

STEP IV:  Immunocompetent T-Cells (those that DON'T bind to self) 
          become "valid detectors"
```

### 1.2 Mapping to Security Context (for IMMUNOS)

| Biological Component | NegSl-AIS Component | IMMUNOS Security Equivalent |
|---------------------|---------------------|----------------------------|
| Self Antigens | Training samples from target class | Known-good code patterns |
| Non-Self Antigens | Samples from other classes | Malicious/anomalous code |
| Naïve T-Cells | Randomly generated detectors | Pattern signatures |
| Thymus | Training environment | Signature training pipeline |
| Immunocompetent T-Cells | Valid detectors | Verified threat detectors |
| Blood Stream | Production environment | Live code scanning |

---

## 2. DETECTOR GENERATION ALGORITHM

### 2.1 Validation Rule (Figure 8)

A detector is **valid** if and only if:

```
Detector = {
    Valid,   if R_q > R_self
    Invalid, if R_q < R_self
}
```

Where:
- **R_q** = Euclidean distance between candidate detector center and nearest self-sample
- **R_self** = Predefined threshold radius for self-samples

### 2.2 Mathematical Formulation

**Euclidean Distance Calculation:**

```python
R_q = min(||Self_i, d_j||) for i = 1 to N

R_q = sqrt(sum((Ω_k^i - d_k^j)^2) for k = 1 to K)
```

Where:
- `Ω^i` = Center point of self-sample i
- `d^j` = Center point of candidate detector j
- `K` = Number of dimensions in feature space
- `N` = Number of self-samples

**Detector Radius:**
```
r^j = R^q - R^self
```

### 2.3 Training Pseudocode

```python
# TRAINING PHASE
def train_detectors(self_samples, R_self, num_detectors):
    """
    Generate valid detectors using negative selection
    
    Input:
        self_samples: m x n matrix of training samples for target class
        R_self: threshold radius for self-samples
        num_detectors: number of candidate detectors to generate
    
    Output:
        D: set of valid detectors
    """
    D = set()  # Valid detectors
    R = []     # Candidate detectors
    
    # Step 1: Generate random candidate detectors
    for i in range(num_detectors):
        r_i = [random.uniform(0, 1) for j in range(n)]
        R.append(r_i)
    
    # Step 2: Validate each detector against self-samples
    for sample in self_samples:
        for detector in R:
            R_q = euclidean_distance(sample, detector)
            if R_q > R_self:
                D.add(detector)  # Valid - doesn't bind to self
    
    # Step 3: Remove duplicates
    D = remove_duplicates(D)
    
    return D
```

### 2.4 Testing Pseudocode

```python
# TESTING PHASE
def classify_sample(test_sample, valid_detectors, R_self):
    """
    Classify a test sample using valid detectors
    
    Input:
        test_sample: feature vector to classify
        valid_detectors: set of valid detectors from training
        R_self: same threshold used in training
    
    Output:
        classification: "self" or "non-self"
    """
    for detector in valid_detectors:
        R_q = euclidean_distance(test_sample, detector)
        if R_q < R_self:
            return "self"  # Sample matches this class
    
    return "non-self"  # Sample doesn't match
```

---

## 3. OPTIMAL PARAMETERS

### 3.1 Detector Configuration (Figure 10)

| Class | Number of Detectors | R_self Threshold | Training Acc | Hold-out Test Acc |
|-------|---------------------|------------------|--------------|-------------------|
| Low Arousal (LA) | 15 | 0.89 | 99.72% | 86.08% |
| High Arousal (HA) | 15 | 0.91 | 99.61% | 96.48% |
| Low Valence (LV) | 25 | 1.31 | 99.86% | 98.63% |
| High Valence (HV) | 20 | 1.33 | 99.76% | 95.02% |

### 3.2 Minimum Generalization Errors (Figure 11)

| Class | Min Generalization Error |
|-------|-------------------------|
| Low Arousal | 0.000506064 |
| High Arousal | 0.001095896 |
| Low Valence | 0.001396648 |
| High Valence | 0.000149223 |

---

## 4. DATA PREPROCESSING PIPELINE

### 4.1 Pipeline Overview (Figure 4)

```
Raw Data → Segmentation → Modality-Specific Preprocessing → 
Data Stratification → Re-referencing → Windowing → Clean Data
```

### 4.2 Modality-Specific Preprocessing

| Modality | Filter Type | Parameters |
|----------|-------------|------------|
| EEG | Low-pass Butterworth | Cutoff: 30 Hz |
| ECG | Band-pass | 0.5 Hz - 30 Hz |
| GSR | Detrending | Remove slow-varying trends |
| Respiration | Gaussian + Z-score | Normalization |
| Body Temperature | Low-pass + Zero-mean | Normalization |

### 4.3 Windowing Parameters

- **Window Size:** 6 seconds
- **Sampling Frequency:** 256 Hz
- **Samples per Window:** 1536 × N (N = number of channels)
- **Stride:** Unit stride with overlap

### 4.4 Channel Selection

| Modality | Selected Channels | Count |
|----------|------------------|-------|
| EEG | FP1, AF3, F3, FP2, AF4, F4 | 6 |
| ECG | EXG1, EXG2, EXG3 | 3 |
| GSR | GSR1 | 1 |
| Respiration | Resp | 1 |
| Body Temperature | Temp | 1 |
| **Total** | | **12** |

---

## 5. FEATURE EXTRACTION

### 5.1 Extracted Features by Modality

**EEG Features:**
- Waveform length
- Energy
- Entropy
- Power Spectral Density (PSD)
- Mel Spectrogram (window=42, fs=256Hz, Mel bands=2, freq range=0-30Hz)
- Discrete Wavelet Transform (Daubechies db4 - alpha, beta, theta bands)

**ECG Features:**
- R-peak amplitude: `R_p = max(V_R)`
- Duration: `D_ECG = t_E - t_S`
- Heart Rate Variability (SDNN): `sqrt(1/(n-1) * sum((R_i - R')^2))`

**GSR Features:**
- Amplitude: `GSR = G(t) - G_B`
- Slope: `m(t) ≈ G(t+1) - G(t)`

**Respiration Features:**
- Baseline amplitude: `BaseR = (1/T) * integral(R(t)dt)`
- Peak-to-peak amplitude: `PTP_R = max(R(t)) - min(R(t))`

**Body Temperature Features:**
- Amplitude variability: `AV = (1/N) * sum(|T(t_n) - μ|)`

### 5.2 Statistical Features (All Modalities)

| Feature | Formula |
|---------|---------|
| Standard Deviation | `SD = sqrt(1/(n-1) * sum((x_i - x')^2))` |
| Skewness | `(n/((n-1)(n-2))) * sum(((x_i - x')/SD)^3)` |
| Zero Crossing Rate | `(1/n) * sum(\|Sign(x_i) - Sign(x_{i+1})\|)` |
| Kurtosis | `(1/n) * sum(((x_i - μ)/SD)^4) - 3` |
| Waveform Length | `sum(\|x_{i+1} - x_i\|)` |
| Energy | `sum(\|x_i\|^2)` |
| Entropy | `-sum(P(x_i) * log2(P(x_i)))` |

### 5.3 LSTM Feature Extraction

```python
LSTM_CONFIG = {
    'input_shape': (samples, 1536, 47),  # (batch, timesteps, features)
    'num_layers': 2,
    'units': {
        'layer_1': 64,
        'layer_2': 32
    },
    'return_sequence': True,
    'optimizer': 'Adam',
    'activation': 'sigmoid',
    'loss': 'binary_crossentropy',
    'window_size': (1536, 12)  # 12 selected channels
}
```

---

## 6. MODALITY BIASING & FEATURE FUSION

### 6.1 Modality Weights (Figure 6)

Based on unimodal classification accuracies:

| Modality | Unimodal Accuracy | Weight |
|----------|-------------------|--------|
| EEG | 58.25% | 0.28 |
| ECG | 53.85% | 0.26 |
| Respiration Amplitude | 46.38% | 0.25 |
| GSR | 28.89% | 0.14 |
| Body Temperature | 27.99% | 0.07 |
| **Total** | | **1.00** |

### 6.2 Feature Normalization

```python
# Min-Max Normalization (most modalities)
normalized_value = (X - min(X)) / (max(X) - min(X))

# Z-score normalization (respiration)
z_score = (X - mean(X)) / std(X)

# Zero-mean normalization (body temperature)
zero_mean = X - mean(X)
```

### 6.3 Final Feature Vector Dimensions

| Class | Samples | Features | Dimension |
|-------|---------|----------|-----------|
| Low Arousal | 3,735 | 7,690 | 3735 × 7690 |
| High Arousal | 3,693 | 7,690 | 3693 × 7690 |
| Low Valence | 3,409 | 7,690 | 3409 × 7690 |
| High Valence | 4,019 | 7,690 | 4019 × 7690 |
| **Total** | **14,856** | **7,690** | **14856 × 7690** |

---

## 7. DATA SPLIT STRATEGY (Figure 9)

### 7.1 Overall Split

```
Total Data
├── 70% Training/Validation (Subjects 1-8, 10-11, 13-14, 16-21)
│   ├── 5-Fold Cross Validation
│   │   ├── Fold 1: Val_1 | Trg_1 | Test_Set_1
│   │   ├── Fold 2: Trg_2 | Val_2 | Test_Set_2
│   │   ├── Fold 3: Trg_3 | Val_3 | Test_Set_3
│   │   ├── Fold 4: Trg_4 | Val_4 | Test_Set_4
│   │   └── Fold 5: Trg_5 | Val_5 | Test_Set_5
│   └── Internal Test Sets
└── 30% Hold-out Test (Subjects 22-30)
```

### 7.2 Subject-Independent Split

- **Training Set:** 18 subjects (1-8, 10-11, 13-14, 16-21)
- **Test Set:** 9 subjects (22-30)
- **Note:** Subjects 9, 12, 15 excluded due to technical faults

---

## 8. CLASSIFICATION METRICS

### 8.1 Class-wise Performance (Table 8)

| Class | Precision | Recall | F1-Score | Specificity | NPV | Support |
|-------|-----------|--------|----------|-------------|-----|---------|
| LA | 0.974 | 0.861 | 0.915 | 0.992 | 0.955 | 1121 |
| HA | 0.930 | 0.965 | 0.947 | 0.976 | 0.988 | 1108 |
| LV | 0.922 | 0.986 | 0.953 | 0.975 | 0.996 | 1023 |
| HV | 0.937 | 0.950 | 0.944 | 0.976 | 0.981 | 1206 |

### 8.2 Confusion Matrix Values (Figure 14)

```
              Predicted
           LA    HA    LV    HV
Actual LA  965   79    21    56
       HA  25   1069    5     9
       LV   1     1  1009    12
       HV   0     1    59  1146
```

### 8.3 Generalization Error Formula

```python
E_gen = abs((1/N_trg * sum(L(y'_i, y_i))) - (1/N_test * sum(L(y'_test_i, y_test_i))))
```

Where:
- `N_trg` = number of training samples
- `L()` = loss function
- `y'_i` = predicted value
- `y_i` = actual value

---

## 9. CIRCUMPLEX MODEL OF EMOTION (Figure 1)

### 9.1 Four Quadrants

```
                    High Arousal
                         │
    ┌────────────────────┼────────────────────┐
    │                    │                    │
    │  HIGH AROUSAL      │   HIGH AROUSAL     │
    │  NEGATIVE VALENCE  │   POSITIVE VALENCE │
    │                    │                    │
    │  • Tense           │   • Excited        │
    │  • Angry           │   • Delighted      │
    │  • Frustrated      │   • Happy          │
    │                    │                    │
Neg ├────────────────────┼────────────────────┤ Pos
Val │                    │                    │ Val
    │  LOW AROUSAL       │   LOW AROUSAL      │
    │  NEGATIVE VALENCE  │   POSITIVE VALENCE │
    │                    │                    │
    │  • Depressed       │   • Content        │
    │  • Bored           │   • Relaxed        │
    │  • Tired           │   • Calm           │
    │                    │                    │
    └────────────────────┼────────────────────┘
                         │
                    Low Arousal
```

### 9.2 Binary Classification Threshold

- **Low:** Response < 5
- **High:** Response ≥ 5
- Scale: 1-9 for both arousal and valence

---

## 10. IMPLEMENTATION CONSIDERATIONS FOR IMMUNOS

### 10.1 Direct Mappings

| NegSl-AIS Concept | IMMUNOS Implementation |
|-------------------|----------------------|
| Self Samples | Known-safe code patterns |
| Non-Self Samples | Malicious/suspicious patterns |
| Detector Generation | Training signature database |
| R_self Threshold | Detection sensitivity |
| One-vs-All Classification | Binary safe/unsafe classification |
| Feature Fusion | Multi-scanner result aggregation |
| Modality Biasing | Scanner priority weighting |

### 10.2 Key Algorithms to Implement

1. **Negative Selection Detector Generation**
2. **Euclidean Distance Calculation**
3. **Threshold-based Validation**
4. **Feature Extraction Pipeline**
5. **Modality Biasing/Weighting**
6. **Cross-validation Framework**
7. **Generalization Error Tracking**

### 10.3 Suggested Architecture

```
IMMUNOS Core
├── Detector Engine (Negative Selection)
│   ├── Detector Generation
│   ├── Detector Validation
│   └── Detector Storage
├── Feature Extraction
│   ├── Static Analysis Features
│   ├── Dynamic Analysis Features
│   └── Pattern Matching Features
├── Scanner Fusion
│   ├── Modality Weights
│   └── Result Aggregation
└── Classification
    ├── Self/Non-Self Discrimination
    └── Confidence Scoring
```

---

## 11. REFERENCES

- Original Paper: Umair et al., "Negative selection-based artificial immune system (NegSl-AIS)", Results in Engineering 27 (2025) 106601
- Dataset: MAHNOB-HCI (Imperial College London)
- DOI: https://doi.org/10.1016/j.rineng.2025.106601

---

*Document generated for IMMUNOS implementation reference*
*Last updated: December 2025*
